import type { GlossaryTerm } from "../types";

export const experimentationTerms: GlossaryTerm[] = [
  {
    slug: "multivariate-testing",
    term: "Multivariate Testing",
    definition:
      "An experimentation method that simultaneously tests multiple variables and their combinations to determine which combination of changes produces the best outcome, unlike A/B testing which typically varies a single element at a time.",
    explanation:
      "Multivariate testing (MVT) is a powerful experimental technique that allows growth and product teams to evaluate multiple variables simultaneously within a single experiment. Instead of testing one headline against another in isolation, MVT might test three headlines, two hero images, and two call-to-action button colors all at once, creating a matrix of all possible combinations. This approach is particularly valuable for advertising and growth teams because it reveals not only which individual element performs best but also how elements interact with each other. A headline that performs well with one image might underperform with another, and these interaction effects are invisible to sequential A/B tests. For teams optimizing landing pages, email campaigns, or ad creatives, MVT provides a more complete picture of what drives conversion.\n\nThe methodology behind MVT relies on factorial experimental design. If you have three variables with 2, 3, and 2 levels respectively, you create a full factorial design with 2 x 3 x 2 = 12 treatment combinations. Each user is randomly assigned to one combination, and the outcome metric is measured across all cells. Statistical analysis then decomposes the overall effect into main effects (the independent contribution of each variable) and interaction effects (how variables modify each other's impact). Tools like Optimizely, VWO, and Google Optimize support MVT natively, handling the traffic allocation and statistical analysis. The key formula involves an ANOVA-style decomposition where the total variance in the outcome is partitioned into variance explained by each factor and their interactions. For digital experiments, the analysis typically uses regression models with indicator variables for each factor level and their cross-products to estimate interaction terms.\n\nMVT should be used when you suspect that multiple page elements interact to influence user behavior, and when you have sufficient traffic to power a multi-cell experiment. The primary pitfall is underestimating the sample size requirement: with 12 combinations, you need roughly 12 times the traffic of a simple A/B test to achieve the same statistical power per cell. This makes MVT impractical for low-traffic pages or rare conversion events. A common alternative is fractional factorial design, which tests a strategically chosen subset of combinations to estimate main effects and key interactions with fewer cells. Teams should also beware of analyzing too many metrics across too many cells, which inflates false discovery rates. When traffic is limited, sequential A/B tests that build on each other's learnings may be more practical than a single large MVT, though they miss interaction effects.\n\nAdvanced practitioners use Taguchi methods or fractional factorial designs to reduce the number of required combinations while still estimating the most important effects. Bayesian MVT approaches, implemented in platforms like Kameleoon, can reach conclusions faster by continuously updating posterior distributions for each combination. Another emerging technique is using contextual bandits within an MVT framework, dynamically allocating more traffic to promising combinations while still exploring undersampled cells. For advertising teams running creative optimization across multiple channels, automated MVT with machine learning-based traffic allocation represents the state of the art, where systems like Meta Advantage+ and Google Performance Max essentially run continuous multivariate experiments across creative elements, audiences, and placements simultaneously.",
    category: "experimentation",
    relatedTerms: ["split-testing", "factorial-design", "power-analysis"],
    relatedPosts: [],
  },
  {
    slug: "split-testing",
    term: "Split Testing",
    definition:
      "The practice of randomly dividing users into two or more groups and exposing each group to a different version of a product experience to measure which version performs better on a target metric, commonly known as A/B testing.",
    explanation:
      "Split testing, often used interchangeably with A/B testing, is the foundational method of online experimentation. Users arriving at a product or page are randomly assigned to one of two or more variants, typically a control (the current experience) and one or more treatments (modified experiences). By comparing the outcome metrics across groups, teams can establish whether a change causes an improvement in user behavior. For growth and advertising teams, split testing is the gold standard for making data-driven decisions because it controls for confounding variables through randomization, establishing causality rather than mere correlation. Every major tech company from Google to Netflix relies on split testing infrastructure to evaluate thousands of product changes per year.\n\nThe mechanics of split testing involve several critical components. First, a randomization mechanism assigns users to variants, typically using a hash of a persistent user identifier to ensure consistent assignment across sessions. The hash function should produce uniform distribution across buckets, and platforms like Statsig, LaunchDarkly, and Eppo handle this automatically. Second, the treatment is applied based on assignment, whether that means showing a different UI, applying different pricing, or changing an algorithm's parameters. Third, outcome metrics are collected for each group and compared using statistical hypothesis testing. The standard approach uses a two-sample t-test or z-test for continuous and proportion metrics respectively, with the null hypothesis that there is no difference between groups. The test produces a p-value representing the probability of observing a difference as large as measured if the null hypothesis were true. A p-value below the significance threshold (typically 0.05) leads to rejecting the null hypothesis and concluding the treatment has an effect.\n\nSplit testing should be the default method for evaluating any product change where randomized assignment is feasible and ethical. Common pitfalls include insufficient sample size leading to underpowered tests, peeking at results before the planned sample size is reached (which inflates false positive rates), contamination between groups when users interact with each other, and failing to account for multiple comparisons when testing many metrics or variants. Alternatives include quasi-experimental methods like difference-in-differences or synthetic control when randomization is not possible, such as when testing a change that affects all users in a geographic region. Teams should maintain a clear experiment hypothesis, pre-register their primary metric and analysis plan, and avoid post-hoc metric fishing that finds spurious effects.\n\nAdvanced split testing practices include using CUPED or similar variance reduction techniques to increase statistical power without larger samples, implementing sequential testing procedures that allow valid early stopping, and running experiments on the randomization unit that matches the interference structure. For marketplace and social products where users influence each other, cluster-randomized or switchback designs may be necessary. Modern experimentation platforms increasingly support automated experiment analysis with guardrail metrics that flag experiments degrading critical business metrics even when they improve the target metric. The trend toward experiment democratization means more teams run more experiments, making standardized tooling and statistical rigor even more important to prevent the organizational false discovery rate from ballooning.",
    category: "experimentation",
    relatedTerms: ["multivariate-testing", "holdout-testing", "power-analysis"],
    relatedPosts: [],
  },
  {
    slug: "holdout-testing",
    term: "Holdout Testing",
    definition:
      "An experimental design where a small percentage of users are permanently excluded from receiving a new feature or set of features, serving as a long-term control group to measure the cumulative impact of product changes over time.",
    explanation:
      "Holdout testing addresses a fundamental challenge in experimentation programs: while individual A/B tests measure the incremental impact of single changes, teams rarely measure whether the sum of all changes over months or years actually delivers the expected cumulative benefit. A holdout group, typically 1-5% of users, remains on an older version of the product for an extended period, providing a stable baseline against which the aggregate impact of all shipped changes can be measured. For growth teams, holdout tests are essential for validating that the experimentation program as a whole is delivering value and that the cumulative effect of many small wins actually materializes in long-term metrics like retention and revenue.\n\nImplementing a holdout test requires careful infrastructure planning. The holdout group must be defined at the randomization unit level (usually user ID) and persist across all experiments. When a new feature ships after winning an A/B test, the holdout group does not receive it. This means the experimentation platform must support layered assignment where holdout membership takes precedence over individual experiment assignments. Platforms like Statsig and Eppo provide built-in holdout group management. The analysis compares key business metrics between the holdout group and the rest of the user population over time, using the same statistical methods as standard A/B tests but with the holdout as the control. Because the holdout group is typically small, the analysis has lower statistical power for detecting small effects, but since the expected cumulative effect should be large, this is usually acceptable. Teams should track a comprehensive set of metrics including engagement, retention, revenue, and performance indicators.\n\nHoldout tests should be established when an experimentation program matures to the point where dozens of experiments are shipping per quarter. The primary use case is auditing the experimentation program itself: if the holdout group performs similarly to the rest of the population despite dozens of winning experiments being shipped, it signals that either the individual experiment analyses are flawed (perhaps due to peeking or metric gaming) or that the effects are not durable. Common pitfalls include making the holdout group too large, which limits the number of users benefiting from improvements, or too small, which reduces analytical power. Another challenge is managing the user experience for the holdout group, which over time receives an increasingly degraded product. Teams must decide on a refresh cadence, typically every 6-12 months, where the current holdout is dissolved and a new random sample is selected.\n\nAdvanced holdout strategies include maintaining multiple holdout groups at different refresh points to create a continuous measurement baseline, using propensity score weighting to adjust for any compositional drift that occurs if holdout users churn at different rates, and implementing partial holdouts that exclude users from specific categories of changes while receiving others. Some organizations use holdouts to validate machine learning model improvements specifically, keeping a group on the previous model version. The concept extends to advertising, where brand lift holdout studies withhold ads from a random group to measure the true incremental impact of advertising beyond organic behavior. Netflix famously uses holdout groups to validate that their personalization algorithms collectively deliver substantial engagement gains, publishing research showing the holdout approach catches cases where individual test results overstate long-term impact.",
    category: "experimentation",
    relatedTerms: ["split-testing", "long-running-experiment", "guardrail-metric-testing"],
    relatedPosts: [],
  },
  {
    slug: "power-analysis",
    term: "Power Analysis",
    definition:
      "A statistical calculation performed before an experiment to determine the minimum sample size required to detect a meaningful effect with a specified probability, balancing the risk of false negatives against practical constraints like traffic and experiment duration.",
    explanation:
      "Power analysis is the critical planning step that determines whether an experiment is worth running before any resources are committed. Statistical power is the probability that a test will correctly detect a true effect when one exists, conventionally set at 80% or higher. A power analysis takes as inputs the desired significance level (alpha, typically 0.05), the minimum detectable effect size the team cares about, the baseline metric value and its variance, and the desired power level, then outputs the required sample size per variant. For growth teams, power analysis prevents two costly mistakes: running experiments that are too small to detect realistic effects (wasting time on inconclusive results) and running experiments far longer than necessary (delaying the rollout of winning changes).\n\nThe standard power analysis formula for a two-sample proportion test is n = (Z_alpha/2 + Z_beta)^2 * (p1(1-p1) + p2(1-p2)) / (p1 - p2)^2, where p1 and p2 are the baseline and expected treatment proportions, Z_alpha/2 is the critical value for the significance level, and Z_beta is the critical value for the desired power. For continuous metrics, the formula uses the pooled standard deviation instead of the proportion variances. Most experimentation platforms like Statsig, Eppo, and Optimizely include built-in power calculators that automate this computation. The practical workflow involves specifying the primary metric, looking up its current baseline value and variance from historical data, specifying the minimum effect size worth detecting (the minimum detectable effect or MDE), and computing the required sample size. Dividing this by the daily traffic rate gives the expected experiment duration. If the duration is impractically long, teams can either increase the MDE (accept that only larger effects will be detected), use variance reduction techniques like CUPED, choose a more sensitive metric, or increase the significance level.\n\nPower analysis should be performed for every experiment before launch, and experiments should not be launched if they cannot achieve adequate power within a reasonable timeframe. A common pitfall is performing power analysis on the overall conversion rate when the change only affects a subset of users; in such cases, the analysis should be restricted to the affected population using triggered analysis. Another mistake is using unrealistically small MDEs to justify large experiments when the team would ship the change even with a smaller effect. Teams should also account for multiple variants: a test with three treatment arms requires roughly 50% more total sample than a two-arm test for the same per-comparison power. Sequential testing designs can reduce the expected sample size by allowing early stopping when effects are large, but the power analysis for these designs uses different formulas that account for the multiple interim analyses.\n\nAdvanced power analysis considerations include accounting for clustered randomization, where the effective sample size is reduced by the intracluster correlation coefficient; handling ratio metrics where the variance depends on both numerator and denominator; and using simulation-based power analysis for complex metrics that do not follow standard distributional assumptions. Bayesian power analysis, sometimes called assurance analysis, computes the probability that the posterior credible interval will exclude zero given a prior distribution over the true effect size, which more naturally incorporates uncertainty about the expected effect. For organizations running many experiments, meta-analytic power analysis uses the distribution of observed effects from past experiments to inform realistic MDE assumptions, often revealing that most true effects are smaller than teams typically assume.",
    category: "experimentation",
    relatedTerms: ["effect-size", "minimum-detectable-effect", "type-ii-error"],
    relatedPosts: [],
  },
  {
    slug: "effect-size",
    term: "Effect Size",
    definition:
      "A quantitative measure of the magnitude of a treatment's impact, expressed either as an absolute difference between groups or as a standardized metric like Cohen's d that allows comparison across different experiments and metrics.",
    explanation:
      "Effect size measures how large a difference a treatment produces, separate from whether that difference is statistically significant. Statistical significance tells you whether an effect exists; effect size tells you whether it matters. A tiny effect can be statistically significant with a large enough sample, while a meaningful effect can fail to reach significance in an underpowered test. For growth and advertising teams, effect size is ultimately what determines business impact. A statistically significant 0.01% improvement in click-through rate may be real but commercially irrelevant, while a 5% improvement in conversion rate could be worth millions in annual revenue. Understanding effect sizes helps teams prioritize which experiments to run, set appropriate sample sizes, and evaluate whether winning experiments justify implementation costs.\n\nEffect size can be expressed in several ways depending on context. The absolute effect size is simply the difference in metric values between treatment and control, such as a 2 percentage point increase in conversion rate from 10% to 12%. The relative effect size expresses this as a percentage change from baseline: a 20% relative lift in the example above. Standardized effect sizes like Cohen's d divide the absolute difference by the pooled standard deviation, producing a unit-free measure that allows comparison across different metrics. Cohen's d = (mean_treatment - mean_control) / pooled_SD. Convention labels d = 0.2 as small, d = 0.5 as medium, and d = 0.8 as large, though these benchmarks were developed for behavioral science and may not apply directly to digital experimentation where even small standardized effects can have large business impact at scale. For proportion metrics, the effect size can also be expressed using the odds ratio or risk ratio. Experimentation platforms typically report both absolute and relative effect sizes along with confidence intervals.\n\nTeams should focus on the practical significance of effect sizes rather than relying solely on p-values. A useful framework is defining the minimum effect size of interest (MESOI) before the experiment: what is the smallest improvement that would justify shipping the change given its implementation and maintenance costs? This feeds directly into power analysis and helps teams avoid celebrating statistically significant but trivially small effects. Common pitfalls include confusing relative and absolute effect sizes in communication (a 50% relative increase from 0.2% to 0.3% sounds impressive but is tiny in absolute terms), ignoring the confidence interval around the point estimate (a point estimate of +5% with a 95% CI of [-1%, +11%] is much less actionable than +5% with CI [+3%, +7%]), and failing to consider effect size heterogeneity across user segments.\n\nAdvanced considerations include using effect size distributions from an organization's experiment history to calibrate expectations. Research by Ron Kohavi and others shows that most A/B tests in industry produce small effects (median around 0-2% relative change), which has important implications for power analysis and experiment prioritization. Meta-analysis techniques aggregate effect sizes across related experiments to produce more precise estimates of a treatment category's typical impact. For Bayesian analysis, the effect size prior should reflect realistic expectations based on historical data rather than uninformative priors. Heterogeneous treatment effect analysis, using methods like causal forests, reveals how effect size varies across user segments, often finding that an overall small effect masks large positive effects for some users and negative effects for others.",
    category: "experimentation",
    relatedTerms: ["power-analysis", "minimum-detectable-effect", "practical-significance", "confidence-interval"],
    relatedPosts: [],
  },
  {
    slug: "confidence-interval",
    term: "Confidence Interval",
    definition:
      "A range of values, derived from sample data, that is expected to contain the true population parameter with a specified probability, providing both an estimate of the treatment effect and the precision of that estimate.",
    explanation:
      "A confidence interval (CI) provides far more information than a simple point estimate or p-value by quantifying the uncertainty around a measured effect. A 95% confidence interval means that if the experiment were repeated many times, 95% of the computed intervals would contain the true effect. For growth teams, confidence intervals are essential for making informed ship/no-ship decisions because they communicate both the likely magnitude and the range of plausible values for a treatment effect. A point estimate of +3% conversion lift tells you the most likely outcome, but the confidence interval [+0.5%, +5.5%] tells you the best and worst realistic scenarios, enabling proper risk assessment and revenue forecasting.\n\nThe standard confidence interval for a difference in means is calculated as (X_bar_treatment - X_bar_control) +/- Z_alpha/2 * SE, where SE is the standard error of the difference, computed as sqrt(s1^2/n1 + s2^2/n2). For proportions, the SE uses the proportion formula sqrt(p1(1-p1)/n1 + p2(1-p2)/n2). The width of the confidence interval is inversely proportional to the square root of the sample size, meaning that quadrupling the sample size halves the interval width. Experimentation platforms like Statsig, Optimizely, and Eppo display confidence intervals prominently in their dashboards. Many platforms also offer Bayesian credible intervals, which have a more intuitive interpretation: a 95% credible interval means there is a 95% probability that the true parameter lies within the interval, given the data and prior. This distinction matters because the frequentist CI's coverage guarantee applies to the procedure across hypothetical repetitions, not to the specific interval computed.\n\nTeams should use confidence intervals as the primary output of experiment analysis rather than relying on binary significant/not-significant conclusions. A result can be not statistically significant but still highly informative: a 95% CI of [-0.5%, +4%] for conversion lift suggests the treatment is unlikely to be harmful and has a good chance of being beneficial, which might justify shipping. Conversely, a statistically significant result with a wide CI that includes trivially small effects may not warrant the implementation investment. Common pitfalls include misinterpreting the confidence level (it is not the probability that the true value is in this specific interval in the frequentist framework), ignoring the width of the interval when making decisions, and not adjusting confidence levels when examining multiple metrics or segments simultaneously. When running multiple comparisons, the family-wise confidence level is lower than the individual interval level.\n\nAdvanced applications include using confidence intervals for equivalence testing, where the goal is to show that a new system performs within an acceptable range of the old one rather than showing superiority. If the entire CI falls within a pre-specified equivalence margin, the treatment is declared equivalent. This is valuable for infrastructure migrations, code refactors, and cost-reduction changes where the goal is to confirm no harm. Confidence intervals also enable more nuanced meta-analysis across experiments, where overlapping CIs from multiple tests can be combined using inverse-variance weighting to produce a pooled estimate with a narrower interval. For sequential experiments that allow early stopping, the confidence intervals must be adjusted using methods like alpha spending functions to maintain valid coverage despite the multiple looks at the data.",
    category: "experimentation",
    relatedTerms: ["effect-size", "type-i-error", "type-ii-error"],
    relatedPosts: [],
  },
  {
    slug: "type-i-error",
    term: "Type I Error",
    definition:
      "The error of incorrectly rejecting a true null hypothesis, also known as a false positive, where an experiment concludes that a treatment has an effect when in reality there is no true difference between treatment and control.",
    explanation:
      "A Type I error occurs when statistical noise in the data creates the appearance of a treatment effect that does not actually exist. The probability of committing a Type I error is controlled by the significance level (alpha), conventionally set at 0.05, meaning a 5% chance of declaring a winner when there is no real difference. For growth and advertising teams, Type I errors are costly because they lead to shipping changes that provide no actual benefit while consuming engineering resources for implementation and creating complexity in the codebase. In an organization running hundreds of experiments per year, a 5% false positive rate means dozens of shipped no-op changes annually, each adding maintenance burden and potentially degrading user experience through unnecessary complexity.\n\nThe mechanism behind Type I errors is straightforward: even when two groups are identical, random sampling variation means their measured metrics will differ slightly. The p-value quantifies how extreme this observed difference is under the null hypothesis. When the p-value falls below alpha by chance (which happens with probability alpha when the null is true), a Type I error results. The formal testing framework involves computing the test statistic T = (effect_estimate) / SE(effect_estimate), comparing it to the critical value from the relevant distribution (normal, t, or chi-squared), and rejecting the null if the test statistic exceeds the critical value. The significance level alpha determines the critical value: for a two-sided test at alpha = 0.05, the critical z-value is 1.96. Multiple factors inflate the actual Type I error rate beyond the nominal alpha: testing multiple metrics, peeking at results before the planned sample size, testing multiple variants, and post-hoc subgroup analyses all increase the chance of at least one false positive.\n\nTeams should be vigilant about factors that inflate Type I error rates beyond the nominal level. The most common inflation source is peeking, where analysts check results repeatedly and stop the experiment when significance is observed. If you check results every day for 30 days, the actual false positive rate can exceed 25% even with alpha = 0.05. Solutions include using sequential testing methods that account for multiple looks, committing to a fixed sample size before launching, or using always-valid p-values. Multiple comparison corrections like Bonferroni (divide alpha by the number of tests) or Benjamini-Hochberg (controls false discovery rate) should be applied when analyzing multiple metrics. For organizations with many teams running experiments, establishing an experiment review board that enforces pre-registration and proper correction procedures is essential for maintaining the credibility of the experimentation program.\n\nAdvanced approaches to Type I error control include using closed testing procedures that maintain strong familywise error rate control while being less conservative than Bonferroni, implementing the Benjamini-Hochberg procedure to control the false discovery rate (FDR) rather than the familywise error rate when many metrics are analyzed, and employing alpha spending functions (O'Brien-Fleming or Pocock boundaries) for sequential monitoring. For Bayesian experiments, the direct analog of Type I error is the probability that the posterior favors the treatment when the null is true, which can be calibrated through simulation. Some organizations adopt a tiered alpha approach: using a stricter threshold (e.g., alpha = 0.01) for high-stakes decisions like pricing changes and a more lenient threshold (e.g., alpha = 0.10) for low-risk UI optimizations, reflecting the asymmetric costs of false positives across decision types.",
    category: "experimentation",
    relatedTerms: ["type-ii-error", "multiple-comparison-correction", "false-discovery-rate", "peeking-problem"],
    relatedPosts: [],
  },
  {
    slug: "type-ii-error",
    term: "Type II Error",
    definition:
      "The error of failing to reject a false null hypothesis, also known as a false negative, where an experiment fails to detect a real treatment effect, concluding there is no difference when one actually exists.",
    explanation:
      "A Type II error occurs when an experiment misses a genuine treatment effect, typically because the sample size is too small to distinguish the signal from noise. The probability of a Type II error is denoted beta, and statistical power (1 - beta) is the complement. With the conventional power target of 80%, there is a 20% chance of missing a real effect of the specified size. For growth teams, Type II errors are arguably more costly than Type I errors in many contexts: they lead to abandoning effective changes, creating a bias toward the status quo and slowing the pace of product improvement. A team that consistently runs underpowered experiments will discard many genuinely beneficial ideas, conclude that experimentation does not work, and potentially abandon the experimentation practice altogether.\n\nType II errors are directly related to statistical power, which depends on four factors: the significance level alpha (higher alpha increases power but also Type I error risk), the sample size (larger samples increase power), the true effect size (larger effects are easier to detect), and the variance of the metric (lower variance increases power). The relationship is captured in the power formula: power = P(reject H0 | H1 is true) = P(Z > Z_alpha/2 - delta*sqrt(n)/(2*sigma)), where delta is the true effect size and sigma is the standard deviation. When an experiment fails to find a significant result, it does not prove the treatment has no effect; it only means the data were insufficient to detect an effect at the specified power level. The confidence interval around the null result is informative: a tight CI around zero suggests the true effect is genuinely small, while a wide CI means the test was simply inconclusive.\n\nTo minimize Type II errors, teams should perform rigorous power analysis before every experiment, targeting 80% or higher power for the minimum effect size of practical interest. When traffic is limited, several strategies can increase power without more users: variance reduction techniques like CUPED can reduce metric variance by 30-50% by controlling for pre-experiment covariates, effectively increasing the sample size. Using more sensitive metrics (e.g., revenue per user instead of conversion rate) can increase the signal-to-noise ratio. Restricting analysis to triggered users who actually encountered the change eliminates noise from unaffected users. Teams should also be cautious about interpreting null results: rather than declaring no effect, report the confidence interval and the minimum effect size the experiment could have detected. This practice prevents the common mistake of shipping a change without measurement because a previous underpowered test showed no significant effect.\n\nAdvanced considerations include the distinction between individual experiment power and portfolio-level power. An organization running many underpowered experiments at 50% power will miss half of all true effects, creating systematic underestimation of what experimentation can deliver. Some teams use meta-analysis of past experiments to estimate the typical effect size in their domain and calibrate power requirements accordingly. Sequential testing designs offer a potential reduction in expected sample size by allowing early stopping for both efficacy and futility, where a futility boundary allows declaring a Type II error early when the accumulating data strongly suggest the effect is negligibly small. Bayesian approaches frame the problem differently, computing the posterior probability that the effect exceeds a meaningful threshold, which directly addresses the decision-relevant question rather than the binary reject/fail-to-reject framework.",
    category: "experimentation",
    relatedTerms: ["type-i-error", "power-analysis", "effect-size", "minimum-detectable-effect"],
    relatedPosts: [],
  },
  {
    slug: "multiple-comparison-correction",
    term: "Multiple Comparison Correction",
    definition:
      "Statistical adjustments applied when testing multiple hypotheses simultaneously to control the overall probability of making at least one Type I error, preventing the inflation of false positive rates that occurs when many tests are conducted.",
    explanation:
      "Multiple comparison correction addresses a fundamental statistical problem: when you test many hypotheses, the probability of at least one false positive grows rapidly even if each individual test is conducted at a low significance level. With 20 independent tests at alpha = 0.05, the probability of at least one false positive is 1 - (0.95)^20 = 0.64, meaning you are more likely than not to find a spurious significant result. For growth teams, this problem is pervasive because experiments typically track dozens of metrics (primary, secondary, and guardrail), analyze multiple user segments, and may test several variants, creating hundreds of implicit hypothesis tests. Without correction, teams will regularly ship changes based on false positives found through what amounts to data mining.\n\nThe most common correction methods fall into two categories: those controlling the familywise error rate (FWER) and those controlling the false discovery rate (FDR). The Bonferroni correction is the simplest FWER method: divide the significance threshold alpha by the number of tests m, so each test uses alpha/m. With 20 tests and alpha = 0.05, each test must achieve p < 0.0025 to be declared significant. While simple and valid, Bonferroni is conservative, especially when tests are correlated (as metrics often are). The Holm-Bonferroni method is a step-down procedure that is uniformly more powerful: order the p-values from smallest to largest, and compare each to alpha/(m-k+1) where k is its rank, stopping at the first non-rejection. For FDR control, the Benjamini-Hochberg procedure is standard: order the p-values, and find the largest k such that p(k) <= k*alpha/m, then reject all hypotheses with p-values at or below p(k). This controls the expected proportion of false discoveries among all discoveries, which is often more appropriate for exploratory analysis.\n\nTeams should decide upfront which correction method to use based on the analysis context. For the primary metric of an experiment, no correction is needed since there is only one test. For secondary and exploratory metrics, FDR control via Benjamini-Hochberg is usually appropriate since the cost of a false discovery is lower and the conservatism of FWER methods would hide real effects. For guardrail metrics where the cost of missing a degradation is high, FWER methods like Holm-Bonferroni are appropriate. Many teams sidestep the problem by designating a single primary metric that determines the ship decision and treating all other metrics as informational, not requiring correction. Common pitfalls include forgetting to count subgroup analyses as additional tests, applying no correction while examining dozens of metrics, and being so conservative with correction that no experiment ever reaches significance.\n\nAdvanced approaches include graphical multiple comparison procedures that encode the logical relationships between hypotheses, allowing alpha to be recycled from rejected hypotheses to remaining ones in a structured way. For example, if the primary metric is significant, its alpha can be reallocated to secondary metrics, increasing their power. Online FDR control methods like LOND and LORD extend FDR control to the setting where hypotheses arrive sequentially over time, which is relevant for organizations analyzing a continuous stream of experiment results. Resampling-based methods like permutation testing and the bootstrap can provide exact or near-exact control without the conservatism of analytic corrections, especially when test statistics are correlated. Modern experimentation platforms like Statsig increasingly implement intelligent correction strategies automatically, applying different correction levels to primary, secondary, and exploratory metrics.",
    category: "experimentation",
    relatedTerms: ["type-i-error", "false-discovery-rate", "peeking-problem"],
    relatedPosts: [],
  },
  {
    slug: "false-discovery-rate",
    term: "False Discovery Rate",
    definition:
      "The expected proportion of false positives among all statistically significant results, offering a less conservative alternative to familywise error rate control that is more appropriate when many hypotheses are tested and some false discoveries are acceptable.",
    explanation:
      "The false discovery rate (FDR) provides a practical middle ground between no multiple testing correction (which floods results with false positives) and strict familywise error rate control (which is so conservative it misses many real effects). While FWER methods control the probability of even one false positive, FDR controls the expected fraction of discoveries that are false. If you declare 20 metrics as significantly affected and the FDR is controlled at 5%, you expect about 1 of those 20 to be a false positive. For growth teams analyzing dozens of metrics per experiment, FDR control is usually the right approach because it acknowledges that some false discoveries are acceptable as long as the majority of declared effects are real, enabling more aggressive exploration without drowning in noise.\n\nThe standard FDR control method is the Benjamini-Hochberg (BH) procedure, introduced in 1995. The algorithm is: (1) order all m p-values from smallest to largest as p(1) <= p(2) <= ... <= p(m); (2) find the largest k such that p(k) <= k * q / m, where q is the target FDR level (e.g., 0.05); (3) reject all hypotheses corresponding to p(1) through p(k). This procedure controls FDR at level q when the tests are independent or positively correlated, which is usually satisfied for experiment metrics. The Benjamini-Yekutieli procedure extends FDR control to arbitrary dependence structures at the cost of being more conservative. In practice, the BH procedure is much less conservative than Bonferroni: with 100 tests and alpha = 0.05, Bonferroni requires p < 0.0005 for each test, while BH might declare significance for any p < 0.02 depending on the p-value distribution, dramatically increasing the discovery rate for real effects.\n\nFDR control is most appropriate when analyzing multiple secondary metrics, performing segment-level analyses, or conducting discovery-oriented research where the goal is to identify promising leads for follow-up rather than making definitive ship decisions. For the primary experiment metric that determines the ship/no-ship decision, standard single-hypothesis testing at the desired alpha level is more appropriate because you want to control the probability of error for that specific decision. Common pitfalls include confusing FDR with the false positive rate (they answer different questions), not reporting which correction was applied (making results non-reproducible), and cherry-picking the correction method after seeing results. Teams should pre-specify their correction strategy in the experiment analysis plan.\n\nAdvanced FDR concepts include the local false discovery rate (lfdr), which estimates the probability that a specific discovery is false rather than controlling the overall rate, and the positive false discovery rate (pFDR), which conditions on making at least one discovery. The q-value, introduced by John Storey, is the FDR analog of the p-value: the minimum FDR at which a result would be declared significant. Adaptive FDR methods estimate the proportion of true nulls from the data and use this to increase power, since fewer true nulls means more lenient thresholds can maintain FDR control. For sequential experimentation where results accumulate over time, online FDR control algorithms like LOND and SAFFRON maintain FDR guarantees in the streaming setting, which is relevant for always-on experimentation platforms that continuously evaluate new experiments.",
    category: "experimentation",
    relatedTerms: ["multiple-comparison-correction", "type-i-error", "peeking-problem"],
    relatedPosts: [],
  },
  {
    slug: "peeking-problem",
    term: "Peeking Problem",
    definition:
      "The statistical inflation of false positive rates that occurs when experimenters repeatedly check experiment results and stop the test as soon as statistical significance is observed, rather than waiting for the pre-determined sample size to be reached.",
    explanation:
      "The peeking problem is one of the most common and damaging statistical errors in online experimentation. When an analyst checks results daily and stops the experiment the first time the p-value drops below 0.05, the actual false positive rate can be dramatically higher than the nominal 5%. Simulations show that daily checking over a 30-day experiment can inflate the false positive rate to 25% or higher. This happens because random fluctuations in the data can temporarily create the appearance of a significant effect that would disappear with more data. For growth teams under pressure to move fast and ship results, the temptation to peek and stop early is strong, but doing so systematically undermines the credibility of the entire experimentation program by flooding the portfolio with false positives.\n\nThe mathematical mechanism behind peeking inflation is rooted in the properties of random walks. Under the null hypothesis, the running z-statistic of a test follows a process that will eventually cross any fixed threshold given enough looks. The probability of crossing the 1.96 threshold (corresponding to alpha = 0.05) at least once during N checks is much higher than 0.05. Formally, the problem is a repeated application of the optional stopping theorem violation: the expected value of the test statistic remains zero, but the maximum over multiple looks has a larger expected absolute value. The probability of at least one significant result in k independent looks is approximately 1 - (1 - alpha)^k, though actual experiment data is correlated across looks, making the exact inflation depend on the checking schedule and accumulation rate. With continuous monitoring, the inflation is even worse because you are effectively performing infinitely many tests.\n\nThe primary solution is sequential testing, which adjusts significance boundaries to account for multiple looks. Group sequential methods like O'Brien-Fleming and Pocock boundaries define spending functions that allocate the total alpha budget across planned interim analyses. O'Brien-Fleming boundaries are very conservative early (requiring very strong evidence to stop) and close to the fixed-sample boundary at the end, while Pocock boundaries distribute alpha more evenly across looks. Always-valid confidence sequences, recently developed by Howard, Ramdas, and others, provide confidence intervals that maintain their coverage guarantee no matter when you look at the data. Modern experimentation platforms increasingly adopt these methods: Eppo uses always-valid sequential testing, Statsig implements group sequential boundaries, and Optimizely offers sequential testing as an option.\n\nAdvanced approaches to the peeking problem include mixture sequential probability ratio tests (mSPRT), which provide always-valid p-values by mixing over a class of alternative hypotheses, and e-values, which offer a more flexible framework for sequential evidence accumulation that composes naturally under optional stopping. Bayesian approaches are inherently immune to the peeking problem in theory because the posterior distribution is valid regardless of the stopping rule, though in practice the operating characteristics (false positive rate, power) still depend on the stopping rule. For organizations that cannot implement sequential testing, pragmatic solutions include setting a minimum experiment duration before any analysis (e.g., one full business cycle), automating the analysis to run only at the pre-specified end date, and requiring experiment review board approval for early stopping. Training all stakeholders on the peeking problem and making it a cultural norm to respect experiment timelines is essential for maintaining experiment integrity.",
    category: "experimentation",
    relatedTerms: ["stopping-rules", "type-i-error", "confidence-interval"],
    relatedPosts: [],
  },
  {
    slug: "stopping-rules",
    term: "Stopping Rules",
    definition:
      "Pre-defined criteria that determine when an experiment should be concluded, including both the conditions for early termination due to clear results and the maximum duration or sample size at which a final analysis is performed.",
    explanation:
      "Stopping rules formalize the decision of when an experiment has collected enough data to reach a conclusion, preventing both premature termination (which inflates false positive rates) and unnecessarily prolonged experiments (which waste time and limit experimentation velocity). A well-designed stopping rule specifies the planned sample size or duration, any interim analysis points where early stopping is permitted, the statistical boundaries for early stopping decisions, and criteria for stopping due to safety concerns. For growth teams, clear stopping rules are essential for maintaining experiment discipline and enabling valid sequential monitoring without compromising statistical rigor.\n\nThe most common framework for stopping rules in online experimentation is group sequential testing. The experiment is planned for a maximum sample size determined by power analysis, with K planned interim analyses at evenly spaced fractions of the maximum sample. At each interim look, the test statistic is compared to adjusted boundaries. The O'Brien-Fleming spending function produces boundaries that are very stringent early and approach the fixed-sample boundary at the final analysis, which aligns with the practical desire to stop early only when evidence is overwhelming. For a 5-look design with overall alpha = 0.05, O'Brien-Fleming boundaries might be z = 4.56, 3.23, 2.63, 2.28, 2.04 at the five looks. The Lan-DeMets alpha spending function generalizes this by allowing flexible analysis timing. These methods are implemented in tools like Statsig, Eppo, and the R package gsDesign. For futility stopping (concluding no effect exists), conditional power or predictive power calculations assess whether, given the data observed so far, the experiment has a reasonable chance of reaching significance if continued to the planned end.\n\nTeams should establish stopping rules before launching any experiment and document them in the experiment analysis plan. The stopping rule should include: the maximum sample size and duration, the schedule of interim analyses (e.g., weekly after a minimum 1-week burn-in), the statistical boundaries for efficacy and futility stopping, and any safety guardrail metrics that trigger immediate stopping if degraded beyond a threshold. Common pitfalls include having no stopping rule at all (leading to indefinite experiments that block traffic), having rules that are too permissive (allowing early stopping based on peeked p-values), and not implementing futility stopping (continuing hopeless experiments that waste traffic). Teams should also consider the business cycle: experiments should run for at least one full week to capture day-of-week effects, and preferably through any known cyclical patterns.\n\nAdvanced stopping rule designs include adaptive sample size re-estimation, where the planned maximum sample is adjusted at an interim analysis based on the observed effect size and variance, allowing the experiment to grow if effects are smaller than expected. Bayesian stopping rules based on posterior probability thresholds (e.g., stop when P(treatment > control | data) > 0.99) offer intuitive interpretation and natural immunity to the peeking problem, though their frequentist operating characteristics should be verified through simulation. For always-on experiments like recommendation algorithm changes, continuous monitoring with always-valid inference provides valid conclusions at any stopping time. Multi-arm stopping rules add complexity: arms can be dropped for futility while promising arms continue, using methods like the Dunnett procedure or response-adaptive randomization that shifts traffic away from underperforming arms.",
    category: "experimentation",
    relatedTerms: ["peeking-problem", "power-analysis", "adaptive-experiment"],
    relatedPosts: [],
  },
  {
    slug: "sample-ratio-mismatch",
    term: "Sample Ratio Mismatch",
    definition:
      "A diagnostic check that detects whether the observed ratio of users in experiment groups matches the expected ratio from the randomization design, where a significant deviation signals a data quality problem that can invalidate experiment results.",
    explanation:
      "Sample ratio mismatch (SRM) is one of the most important diagnostic checks in online experimentation, yet it is frequently overlooked. If an experiment is designed to split traffic 50/50 between control and treatment, the observed counts should be approximately equal, with some random variation. An SRM test uses a chi-squared goodness-of-fit test to determine whether the observed ratio deviates significantly from the expected ratio. A significant SRM (typically p < 0.001 given the large sample sizes involved) indicates that something in the experiment implementation is systematically biasing which users end up in which group, which violates the fundamental assumption of random assignment and can invalidate all causal conclusions. For growth teams, SRM is a canary in the coal mine: it does not tell you what went wrong, but it tells you that something did.\n\nCommon causes of SRM include bugs in the randomization code, differences in page load performance between variants causing differential bot filtering or user abandonment, redirect-based experiments where one variant's redirect fails more often, initialization timing differences where the assignment check happens at different points in the user flow, and browser or client-side caching that affects variant delivery inconsistently. For example, if the treatment variant loads 200ms slower than control, some treatment users may bounce before their visit is logged, creating an SRM where control has more recorded users. Another common cause is interaction between experiments: if experiment A's treatment causes some users to never reach experiment B's assignment point, experiment B will show an SRM among users exposed to experiment A's treatment. The SRM test is computed as chi_squared = sum((observed_i - expected_i)^2 / expected_i) across all groups, compared to a chi-squared distribution with k-1 degrees of freedom.\n\nEvery experiment should include an automated SRM check that runs continuously and alerts when a mismatch is detected. The check should be performed early in the experiment (often within the first day) so that problematic experiments can be stopped before they waste more traffic. When an SRM is detected, the experiment results should be considered invalid until the root cause is identified and resolved. Common debugging steps include checking for differential logging between variants, examining bot traffic patterns, verifying that the randomization hash function produces uniform distribution, checking for interactions with other concurrent experiments, and examining whether variant-specific errors or timeouts could cause differential data loss. Teams should never try to fix an SRM by reweighting or adjusting the data; the goal is to identify and fix the underlying implementation bug.\n\nAdvanced SRM analysis includes checking not just the overall ratio but also the ratio over time (a sudden shift may indicate a deployment or configuration change), across different platforms or geographies (a platform-specific SRM points to a client-side implementation issue), and across different user segments. Some experimentation platforms like Statsig and Eppo run SRM checks automatically and flag experiments with mismatches. The concept extends beyond simple two-group experiments: for multi-arm experiments, factorial designs, and layered experiment systems, SRM checks should verify all expected ratios including interaction cells. Research by Fabijan, Dmitriev, and others at Microsoft has documented that SRM affects a significant percentage of experiments at major tech companies and is often the first indicator of subtle bugs that would otherwise silently bias results for months.",
    category: "experimentation",
    relatedTerms: ["split-testing", "randomization-unit", "guardrail-metric-testing"],
    relatedPosts: [],
  },
  {
    slug: "novelty-effect",
    term: "Novelty Effect",
    definition:
      "A temporary change in user behavior caused by the newness of a feature or design change rather than its intrinsic value, where engagement metrics initially spike because users explore the new experience but then decay as the novelty wears off.",
    explanation:
      "The novelty effect is a pervasive threat to experiment validity that causes teams to overestimate the long-term impact of changes. When users encounter something new, whether a redesigned interface, a new feature, or a changed workflow, their behavior changes simply because it is different, not necessarily because it is better. They may click more because they are exploring, spend more time because they are relearning, or engage more because of curiosity. If an experiment is analyzed during this novelty phase, the treatment will appear to outperform the control, but the lift will fade as users habituate to the change. For growth teams, failing to account for novelty effects leads to systematically overoptimistic experiment results and a portfolio of shipped changes that collectively underdeliver on their promised impact.\n\nDetecting novelty effects requires analyzing how the treatment effect changes over time within the experiment. The standard approach is to plot the daily or weekly treatment effect estimate and look for a declining trend. More formally, you can segment users by their exposure date and compare the treatment effect for users who were exposed early versus late in the experiment, or use a regression model that includes an interaction between treatment assignment and days since exposure. If the treatment effect is large in the first few days but diminishes over subsequent weeks, a novelty effect is likely present. Another diagnostic is to compare the treatment effect for new users (who have no baseline expectation and thus no novelty response) versus existing users (who experience the change as new). If the effect is much larger for existing users, novelty is a likely driver. Tools like Statsig allow you to view metric deltas over time within an experiment, making this analysis straightforward.\n\nTo mitigate novelty effects, teams should run experiments long enough for the novelty to wear off, typically at least 2-4 weeks for UI changes. Analyzing only the steady-state period after the initial novelty phase provides a more accurate estimate of long-term impact. For experiments where long duration is impractical, teams can focus on new users as a novelty-free segment, since they have no prior experience to compare against. Another approach is to compare the treatment effect trajectory against historical patterns from similar changes to calibrate expected novelty decay. Teams should be especially cautious about novelty effects when testing visual redesigns, notification changes, or any change to well-established user workflows where users have strong habits.\n\nAdvanced approaches to handling novelty effects include fitting parametric decay models (exponential or power law) to the time-varying treatment effect to extrapolate the steady-state impact, using difference-in-differences designs that compare the pre-post change for treatment users against the same period for control users to remove time trends, and implementing long-running holdout groups that measure the treatment effect months after launch. Some researchers have proposed Bayesian models that explicitly decompose the treatment effect into a novelty component and a permanent component, estimating both simultaneously. The mirror image of the novelty effect is the primacy effect, where users initially resist a change and perform worse with the treatment, but gradually improve as they adapt. Both effects argue for longer experiment durations and careful temporal analysis of treatment effect dynamics.",
    category: "experimentation",
    relatedTerms: ["primacy-effect", "long-running-experiment", "holdout-testing"],
    relatedPosts: [],
  },
  {
    slug: "primacy-effect",
    term: "Primacy Effect",
    definition:
      "A temporary depression in user performance or engagement when encountering a changed experience, caused by the disruption of established habits and mental models, which can make a genuinely beneficial treatment appear harmful in the short term.",
    explanation:
      "The primacy effect, sometimes called the change aversion or learning effect, is the opposite of the novelty effect. When users have established habits and mental models for how a product works, any change, even an objectively better one, disrupts their workflow and temporarily reduces their efficiency. Users may struggle to find relocated features, take longer to complete familiar tasks, or express dissatisfaction with the new experience. If an experiment is analyzed during this adaptation period, a truly superior treatment may appear to perform worse than the control, leading teams to incorrectly reject beneficial changes. For growth teams, the primacy effect creates a systematic bias toward the status quo that can stifle innovation and prevent the adoption of improvements that would benefit users in the long run.\n\nDetecting the primacy effect mirrors the approach for novelty effects but looks for the opposite pattern: a treatment effect that starts negative and improves over time. Plot the daily treatment effect and look for an upward trend. Segment analysis comparing new users (who have no established habits) versus existing users is particularly diagnostic: if new users show a positive treatment effect while existing users show a negative effect that diminishes over time, the primacy effect is the likely explanation. Regression models with a time-since-exposure interaction term can formally test whether the treatment effect improves with user adaptation. The key diagnostic distinction is that novelty effects affect engagement metrics (clicks, visits, time spent) while primacy effects tend to affect efficiency metrics (task completion rate, time to complete, error rate), though there is overlap.\n\nTeams should account for primacy effects when testing changes to established workflows, navigation structures, or interface patterns that users have learned. Running experiments for longer durations (3-4 weeks minimum) allows the adaptation period to pass and reveals the steady-state treatment effect. If time constraints prevent long experiments, teams can weight the analysis toward later periods or toward new user segments. Another strategy is to provide a transition experience, such as tooltips highlighting what changed or a brief walkthrough, which can accelerate adaptation and reduce the magnitude of the primacy dip. However, this changes what you are testing: the treatment plus the transition aid rather than the treatment alone. Teams should document known primacy-sensitive experiments and their adaptation timelines to build institutional knowledge.\n\nAdvanced considerations include using survival analysis methods to model the adaptation process, estimating how long it takes for different user segments to reach steady-state performance with the new experience. Some teams implement graduated rollouts where existing users are given the option to try the new experience before being switched, using the self-selected early adopters as a proxy for steady-state behavior. Causal inference methods like instrumental variables can sometimes separate the primacy effect from the true treatment effect by exploiting exogenous variation in the timing or intensity of exposure. The interaction between primacy and novelty effects is also important: some changes may simultaneously increase exploration (novelty) while decreasing efficiency (primacy), creating complex time-varying treatment effects that require careful decomposition to interpret correctly.",
    category: "experimentation",
    relatedTerms: ["novelty-effect", "long-running-experiment", "holdout-testing"],
    relatedPosts: [],
  },
  {
    slug: "factorial-design",
    term: "Factorial Design",
    definition:
      "An experimental design that simultaneously tests all possible combinations of two or more factors, each with multiple levels, enabling the estimation of both individual factor effects and interaction effects between factors in a single experiment.",
    explanation:
      "Factorial design extends simple A/B testing to study multiple factors simultaneously. In a 2x2 factorial design, two binary factors create four treatment cells: neither change, change A only, change B only, and both changes. This design efficiently estimates the main effect of each factor (averaged across levels of the other factors) and the interaction effect (whether the combination is more or less effective than the sum of individual effects). For growth teams, factorial designs are valuable when multiple changes are being considered simultaneously, such as testing both a new headline and a new layout. Rather than running two sequential A/B tests, a factorial design tests everything at once, saves time, and reveals interactions that sequential tests would miss entirely.\n\nThe analysis of a factorial experiment uses a linear model: Y = mu + alpha_i + beta_j + (alpha*beta)_ij + epsilon, where alpha and beta are the main effects and (alpha*beta) is the interaction. In practice, this is implemented as a regression with indicator variables for each factor and their product. The main effect of factor A is estimated by comparing all cells with A=1 to all cells with A=0, regardless of factor B's level, giving each main effect estimate the full sample size rather than the per-cell size. This is the efficiency advantage of factorial designs: with N total users, each main effect is estimated with precision comparable to an N/2-per-group A/B test, not an N/4-per-group test. The interaction term tests whether the effect of A differs depending on the level of B. A significant interaction means the factors are not independent and their combined effect differs from the sum of individual effects. The sample size requirement for detecting interactions is roughly four times that for detecting main effects of the same magnitude, so factorial experiments should be powered based on whether interaction detection is a priority.\n\nFactorial designs should be used when teams want to evaluate multiple independent changes without extending the experimentation timeline, when there is reason to believe factors may interact, or when an organization wants to maximize learning per unit of traffic. Common pitfalls include running factorial designs with too many factors, creating an unmanageable number of cells (a 2^5 design has 32 cells), not having enough traffic to power interaction tests, and interpreting main effects without checking for interactions (which can be misleading if strong interactions exist). Fractional factorial designs address the cell count problem by strategically omitting certain combinations while still estimating main effects and low-order interactions, under the assumption that higher-order interactions are negligible. This is formalized in the sparsity of effects principle.\n\nAdvanced factorial design concepts include resolution, which describes which effects are confounded (aliased) in a fractional design. A Resolution III design can estimate main effects but not separate them from two-factor interactions, while Resolution V can estimate all main effects and two-factor interactions clearly. For digital experimentation, the most practical designs are 2^k full factorials with k = 2 or 3 factors, and 2^(k-p) fractional factorials for k > 3. Some experimentation platforms like Statsig support factorial experiments through their layered experiment infrastructure, where each factor is a separate experiment layer and the platform handles the combinatorial assignment. Response surface methodology extends factorial designs to find optimal continuous parameter values, useful for tuning algorithm parameters like recommendation weights or notification frequency.",
    category: "experimentation",
    relatedTerms: ["multivariate-testing", "split-testing", "latin-square-design"],
    relatedPosts: [],
  },
  {
    slug: "switchback-testing",
    term: "Switchback Testing",
    definition:
      "An experimental design that alternates between treatment and control conditions over time periods within the same unit (such as a geographic region or marketplace), used when user-level randomization is not feasible due to interference or operational constraints.",
    explanation:
      "Switchback testing is a quasi-experimental method designed for situations where traditional user-level A/B testing fails because of interference between users. In a two-sided marketplace like Uber, DoorDash, or Airbnb, changing the matching algorithm for some riders but not others is problematic because both groups compete for the same drivers. If treatment riders get better matches, control riders get worse matches by displacement, violating the stable unit treatment value assumption (SUTVA) that underlies standard A/B testing. Switchback testing solves this by alternating the entire marketplace between treatment and control in time blocks, such as running the treatment algorithm for one hour, then switching to control for the next hour. The treatment effect is estimated by comparing outcome metrics during treatment periods versus control periods, with appropriate adjustments for time trends.\n\nThe methodology involves dividing time into blocks (e.g., hours, days) and randomly assigning each block to treatment or control. The randomization can be stratified by time-of-day and day-of-week to balance known temporal patterns. For geographic markets, different regions can be assigned independently, increasing the effective sample size. The analysis uses a difference-in-means estimator comparing treatment and control periods, with adjustments for temporal autocorrelation and trends. Because adjacent time periods are correlated (demand at 2pm is similar to demand at 3pm), standard error calculations must account for this serial correlation using methods like Newey-West standard errors or cluster-robust standard errors with clustering by time block. The block length involves a tradeoff: shorter blocks increase the number of switches and statistical precision but may not allow the full treatment effect to manifest if there are carryover effects, while longer blocks reduce the number of independent observations.\n\nSwitchback testing should be used when interference between units makes user-level randomization invalid, which is common in marketplaces, ridesharing, delivery, pricing, and any context with shared resources. Common pitfalls include carryover effects where the treatment condition in one period affects outcomes in subsequent control periods (e.g., a pricing change that causes users to stockpile), time-of-day confounding if the treatment and control periods are not balanced across temporal patterns, and insufficient number of switching periods for adequate statistical power. The minimum number of switch periods depends on the desired power and the within-period and between-period variance, but typically at least 50-100 periods are needed for reasonable precision. Alternatives include geo-randomization (assigning entire cities or regions) and synthetic control methods for when switchback is also infeasible.\n\nAdvanced switchback designs include multi-arm switchbacks that test several treatments, crossover designs that expose the same unit to all conditions in a balanced sequence, and adaptive switchback designs that adjust the assignment probability based on accumulating evidence. Interference-aware analysis methods can partially address carryover effects by including lagged treatment indicators in the regression model. Recent research by Bojinov and Shephard at Harvard has developed formal frameworks for switchback experiments that account for both temporal interference and serial correlation, providing valid confidence intervals and hypothesis tests. At companies like Uber, Lyft, and DoorDash, switchback experiments have become a core part of the experimentation toolkit, with dedicated infrastructure for scheduling treatment periods, monitoring real-time metrics during switches, and analyzing results with appropriate temporal adjustments.",
    category: "experimentation",
    relatedTerms: ["cluster-randomization", "crossover-design", "marketplace-experiment"],
    relatedPosts: [],
  },
  {
    slug: "interleaving-test",
    term: "Interleaving Test",
    definition:
      "An experimentation method primarily used for ranking and recommendation systems where results from two algorithms are interleaved into a single list shown to each user, and user interactions with items from each algorithm determine which performs better.",
    explanation:
      "Interleaving tests are a specialized experimental technique designed for evaluating ranking algorithms, search results, recommendation systems, and any system that produces ordered lists of items. Instead of showing half the users algorithm A's results and half algorithm B's results (traditional A/B testing), interleaving shows every user a single merged list that contains items from both algorithms. User interactions like clicks, purchases, or engagement with items are then attributed back to the originating algorithm to determine which algorithm contributed more preferred items. For growth teams working on search, recommendations, or content feeds, interleaving is dramatically more sensitive than traditional A/B testing, often requiring 10-100x fewer users to detect the same effect size.\n\nThe most common interleaving method is Team Draft Interleaving. For each user query, the two algorithms each produce a ranked list. The interleaved list is constructed by alternating between algorithms: a fair coin determines which algorithm contributes the first item, then they alternate, skipping items already included. Each item in the final list is tagged with its source algorithm. When a user clicks on an item, the click counts as a win for that item's algorithm. The test statistic is the proportion of users for whom algorithm A won more clicks versus algorithm B, tested against 0.5 using a binomial test. More sophisticated methods include Balanced Interleaving, which ensures each algorithm contributes equal items regardless of position, and Optimized Interleaving, which maximizes sensitivity by choosing the interleaving that best discriminates between algorithms. These methods are implemented in experimentation frameworks at companies like Netflix, Spotify, and Bing.\n\nInterleaving should be used as a fast screening method for ranking algorithm changes when you need a quick directional signal before committing to a full A/B test. The dramatic sensitivity advantage comes from the within-user comparison: each user serves as their own control, eliminating between-user variance. However, interleaving has important limitations: it only measures preference (which algorithm users prefer) not absolute engagement (whether users engage more overall). An algorithm that shows more clickable but less relevant results might win interleaving tests while decreasing downstream satisfaction. Interleaving also cannot measure system-level effects like changes in total user engagement, session length, or revenue, which require traditional A/B tests. The standard workflow is to use interleaving for fast screening of many candidate algorithms, then validate the winner with a full A/B test that measures business metrics.\n\nAdvanced interleaving methods include Multileaving, which extends interleaving to compare more than two algorithms simultaneously, and Pairwise Preference interleaving, which handles cases where user preferences are relative rather than absolute. For deep learning recommendation systems, interleaving tests need to account for the fact that the merged list may contain items that neither algorithm would have shown in isolation, potentially introducing artifacts. Position bias correction is also important: items higher in the interleaved list receive more clicks regardless of quality, so credit attribution should account for position. Recent work on counterfactual interleaving uses inverse propensity scoring to debias interleaving results when the interleaving policy differs from what would have been shown by either algorithm alone.",
    category: "experimentation",
    relatedTerms: ["split-testing", "bayesian-optimization", "contextual-bandit-experiment"],
    relatedPosts: [],
  },
  {
    slug: "pre-post-analysis",
    term: "Pre-Post Analysis",
    definition:
      "A quasi-experimental method that compares metrics before and after a treatment is applied to the same group, using the pre-treatment period as a baseline to estimate the treatment effect when a randomized control group is not available.",
    explanation:
      "Pre-post analysis is the simplest quasi-experimental design, comparing the same population's metrics before and after an intervention. If conversion rate was 5% in the week before a website redesign and 6% in the week after, the naive pre-post estimate attributes the 1 percentage point increase to the redesign. For growth teams, pre-post analysis is often the only feasible evaluation method for changes that cannot be randomly assigned, such as a complete brand refresh, a platform migration, or a pricing structure change that must apply to all users simultaneously. While pre-post analysis is easy to implement and intuitively appealing, it is the weakest causal inference method because it cannot distinguish the treatment effect from time trends, seasonal patterns, or concurrent changes.\n\nThe basic pre-post analysis computes the average metric in the post period minus the average in the pre period, with statistical significance assessed using a paired t-test or a time series comparison. More sophisticated approaches include interrupted time series analysis, which fits a regression model to the pre-treatment time series and extrapolates the expected trend into the post period, then measures the deviation from this trend as the treatment effect. The model typically includes level change (immediate jump at treatment) and slope change (change in trend after treatment) parameters: Y_t = beta_0 + beta_1*t + beta_2*D_t + beta_3*(t - T)*D_t + epsilon_t, where D_t is an indicator for the post-treatment period and T is the treatment time. This approach accounts for pre-existing trends, making the estimate more credible. Seasonality can be addressed by including calendar indicators or using the same period from the previous year as the baseline.\n\nPre-post analysis should be used when randomization is truly infeasible and when the pre-treatment period provides a stable baseline without major confounding events. The primary pitfall is confounding: any factor that changes coincidentally with the treatment will be attributed to it. If you launch a redesign during the holiday shopping season, the traffic increase from holiday demand will inflate the estimated redesign effect. Mitigation strategies include using a long pre-treatment period to establish stable trends, controlling for known confounders in the regression model, and using a comparison group (even a non-equivalent one) to create a difference-in-differences design that removes shared time trends. Teams should always be explicit about the assumptions underlying pre-post analysis and present results with appropriate caveats about potential confounding.\n\nAdvanced pre-post methods include Bayesian structural time series models (implemented in Google's CausalImpact R package), which use a state space model to forecast the counterfactual post-treatment trajectory and compute the posterior distribution of the treatment effect. This approach can incorporate control time series (metrics from comparable but untreated units) as covariates to improve the counterfactual forecast. Synthetic control methods extend this further by constructing a weighted combination of untreated units that best matches the treated unit's pre-treatment trajectory. For digital experimentation, combining pre-post analysis with regression discontinuity (when treatment assignment has a sharp cutoff) or with instrumental variables (when an exogenous shock drives treatment adoption) can strengthen causal identification substantially.",
    category: "experimentation",
    relatedTerms: ["difference-in-differences", "synthetic-control", "regression-discontinuity"],
    relatedPosts: [],
  },
  {
    slug: "difference-in-differences",
    term: "Difference-in-Differences",
    definition:
      "A quasi-experimental statistical method that estimates a treatment effect by comparing the change in outcomes over time between a group that receives a treatment and a group that does not, removing biases from time-invariant differences between groups and common time trends.",
    explanation:
      "Difference-in-differences (DiD) is one of the most widely used causal inference methods when randomized experiments are not feasible. The core idea is elegant: compare two groups across two time periods. If the treatment group was already different from the control group before the treatment (selection bias) and if both groups would have followed the same trend absent the treatment (the parallel trends assumption), then the difference in their changes over time removes both the selection bias and the common time trend, isolating the treatment effect. For growth and advertising teams, DiD is invaluable for evaluating geo-targeted campaigns, market-level rollouts, policy changes, and any intervention that is applied to identifiable groups rather than randomly assigned individuals.\n\nThe DiD estimator is calculated as: tau = (Y_treatment_post - Y_treatment_pre) - (Y_control_post - Y_control_pre). In regression form: Y_it = beta_0 + beta_1*Treat_i + beta_2*Post_t + beta_3*(Treat_i * Post_t) + epsilon_it, where beta_3 is the DiD estimate of the treatment effect. Treat_i indicates membership in the treatment group, Post_t indicates the post-treatment period, and their interaction captures the differential change. Standard errors should be clustered at the group level to account for within-group correlation over time. With multiple pre and post periods, the regression extends to include time fixed effects and group fixed effects, and the treatment indicator captures the staggered adoption of treatment across groups. Tools for DiD analysis include the R packages did, fixest, and the Python library differences, as well as general regression tools in any statistical software.\n\nDiD should be used when you have a natural comparison group that did not receive the treatment and pre-treatment data for both groups. The critical assumption is parallel trends: absent the treatment, both groups would have followed the same trajectory. This assumption is untestable (since we cannot observe the counterfactual) but can be assessed by examining whether the groups followed parallel trends in the pre-treatment period. If pre-treatment trends diverge, the DiD estimate is biased. Common pitfalls include having too few treated or control units for reliable inference (clustered standard errors require at least 20-30 clusters), violating parallel trends due to differential pre-existing trends, and ignoring anticipation effects where the treatment group changes behavior before the official treatment date. When parallel trends are questionable, alternative methods include synthetic control (which constructs a data-driven comparison group) and changes-in-changes (which relaxes the parallel trends assumption to parallel quantile trends).\n\nAdvanced DiD methods have seen rapid development in recent years. Staggered DiD, where different units adopt treatment at different times, introduces complications because the standard two-way fixed effects estimator can produce biased estimates when treatment effects vary over time. Recent econometric research by Callaway and Sant'Anna, Sun and Abraham, and others provides corrected estimators for staggered adoption settings. Event study designs extend DiD to trace out the treatment effect dynamics over time, plotting the differential change for each period relative to treatment adoption. For advertising and marketing applications, DiD is commonly used in geo-experiments where campaigns are launched in some cities or regions but not others, with platforms like GeoLift (from Meta) providing end-to-end tooling for designing, analyzing, and interpreting geographic DiD experiments.",
    category: "experimentation",
    relatedTerms: ["synthetic-control", "pre-post-analysis", "regression-discontinuity"],
    relatedPosts: [],
  },
  {
    slug: "synthetic-control",
    term: "Synthetic Control",
    definition:
      "A causal inference method that constructs a weighted combination of untreated units to create an artificial control group that closely matches the treated unit's pre-treatment characteristics and trajectory, enabling credible treatment effect estimation when only one or a few units are treated.",
    explanation:
      "The synthetic control method (SCM) addresses a common challenge in quasi-experimental analysis: estimating the impact of a treatment applied to a single unit (a city, a country, a market) when no single untreated unit serves as a good comparison. Instead of relying on one imperfect comparison, SCM constructs a synthetic version of the treated unit by taking a weighted average of multiple untreated units, with weights chosen to minimize the difference between the synthetic and treated unit in pre-treatment outcomes and covariates. For growth and advertising teams, synthetic control is the gold standard for evaluating geo-level interventions like market launches, regional campaigns, or city-level feature rollouts where randomization is infeasible and no single comparison market adequately matches the treated market.\n\nThe SCM algorithm works as follows: let Y_1 be the outcome time series for the treated unit and Y_0 be the matrix of outcomes for J untreated donor units over T_0 pre-treatment periods. The method finds weights W = (w_1, ..., w_J) that minimize ||Y_1_pre - Y_0_pre * W||^2 subject to w_j >= 0 and sum(w_j) = 1. The non-negativity and summing-to-one constraints ensure the synthetic control is a convex combination, preventing extrapolation. The treatment effect at each post-treatment time point is estimated as the gap between the treated unit's actual outcome and the synthetic control's predicted outcome. Statistical inference typically uses permutation-based methods: apply the same SCM procedure to each donor unit as if it were treated, generating a distribution of placebo effects. If the treated unit's effect is extreme relative to the placebo distribution, the effect is considered significant. Meta's GeoLift package and Google's CausalImpact implement variants of synthetic control for marketing and advertising applications.\n\nSynthetic control should be used when a treatment is applied to one or a few units at the aggregate level, there is a pool of similar untreated units available as donors, and sufficient pre-treatment data exists to assess the quality of the synthetic match. The key quality diagnostic is the pre-treatment fit: if the synthetic control closely tracks the treated unit's outcomes in the pre-treatment period, it is reasonable to expect it would continue to do so in the absence of treatment. Common pitfalls include poor pre-treatment fit (which undermines the counterfactual), too few donor units (which limits the ability to construct a good match), donor units that are indirectly affected by the treatment (spillover), and overfitting to pre-treatment noise by using too many predictors. Teams should also be cautious about using SCM for short pre-treatment periods or highly volatile time series where the pre-treatment fit may be misleadingly good.\n\nAdvanced extensions of synthetic control include the augmented synthetic control method (ASCM) by Ben-Aronow and others, which combines SCM with an outcome model to correct for imperfect pre-treatment fit. The penalized synthetic control uses ridge or elastic net regularization to improve stability when there are many donor units. For multiple treated units, the synthetic difference-in-differences (SDID) method by Arkhangelsky and others combines the strengths of DiD and SCM, using synthetic control-style weighting along both the unit and time dimensions. Bayesian synthetic control methods provide posterior distributions over the treatment effect and naturally quantify uncertainty. In the advertising industry, tools like Meta's GeoLift and Google's Matched Markets use synthetic control principles to design and analyze geo-experiments, automatically selecting treatment and control markets and computing power analysis for geographic experiments.",
    category: "experimentation",
    relatedTerms: ["difference-in-differences", "pre-post-analysis", "cluster-randomization"],
    relatedPosts: [],
  },
  {
    slug: "regression-discontinuity",
    term: "Regression Discontinuity",
    definition:
      "A quasi-experimental design that exploits a sharp cutoff in a continuous assignment variable to estimate causal effects, comparing units just above and just below the threshold where treatment assignment changes discontinuously.",
    explanation:
      "Regression discontinuity (RD) design leverages situations where treatment is assigned based on whether a continuous variable crosses a threshold. For example, users who score above a certain engagement threshold might receive a premium feature, or customers whose spending exceeds a cutoff might enter a loyalty tier. The key insight is that units just above and just below the cutoff are nearly identical in all respects except their treatment status, creating a local randomized experiment around the threshold. For growth teams, RD is valuable for evaluating policies and features that are triggered by user characteristics crossing thresholds, such as credit limits, loyalty tiers, algorithmic scores, or usage-based feature access.\n\nThe RD analysis plots the outcome variable against the running variable (the continuous assignment variable) and looks for a discontinuous jump at the cutoff. The treatment effect is estimated as the difference in the regression functions at the threshold: tau = lim(x->c+) E[Y|X=x] - lim(x->c-) E[Y|X=x], where c is the cutoff. In practice, this is estimated by fitting local polynomial regressions on each side of the cutoff using data within a bandwidth around the threshold. The choice of bandwidth involves a bias-variance tradeoff: narrower bandwidths reduce bias by using more comparable units but increase variance by using fewer observations. Optimal bandwidth selection methods, like those implemented in the rdrobust R and Stata packages, balance this tradeoff using mean squared error minimization. The regression model for a sharp RD is: Y_i = alpha + beta*D_i + f(X_i - c) + g(X_i - c)*D_i + epsilon_i, where D_i = 1(X_i >= c) and f and g are flexible functions of the centered running variable.\n\nRD should be used when treatment assignment is determined by a known, measurable cutoff in a continuous variable and when units cannot precisely manipulate their position relative to the cutoff. The main assumption is continuity: all other factors affecting the outcome vary smoothly through the cutoff, so any discontinuity in the outcome is attributable to the treatment. This assumption is violated if units can precisely manipulate the running variable to sort above or below the threshold (e.g., if users can see their score and game the system to exceed the cutoff). McCrary's density test checks for manipulation by testing whether the density of the running variable is continuous at the cutoff; a jump in density suggests sorting. Other common pitfalls include using a bandwidth that is too wide (introducing bias from non-comparable units), not checking sensitivity to bandwidth choice, and over-extrapolating the local treatment effect to the full population.\n\nAdvanced RD concepts include fuzzy RD, where crossing the threshold increases the probability of treatment but does not guarantee it (e.g., being above the cutoff makes users eligible for a feature but not all eligible users activate it). Fuzzy RD uses the cutoff as an instrumental variable for treatment, estimating a local average treatment effect for compliers. Geographic RD exploits spatial boundaries, such as the border between regions with different policies. Regression kink design (RKD) looks for changes in the slope rather than the level of the outcome-running variable relationship, useful when the treatment intensity changes at the kink point rather than the treatment status. For digital experimentation, RD can be combined with other methods: a change in an algorithmic scoring threshold creates an RD opportunity, while the algorithm itself can be evaluated with an A/B test, providing complementary evidence about the system's impact.",
    category: "experimentation",
    relatedTerms: ["difference-in-differences", "propensity-score-matching", "pre-post-analysis"],
    relatedPosts: [],
  },
  {
    slug: "propensity-score-matching",
    term: "Propensity Score Matching",
    definition:
      "A statistical method that reduces selection bias in observational studies by matching treated and untreated units that have similar probabilities (propensity scores) of receiving the treatment, creating a pseudo-randomized comparison.",
    explanation:
      "Propensity score matching (PSM) addresses the fundamental challenge of observational studies: when treatment is not randomly assigned, treated and untreated groups may differ systematically in ways that confound the treatment effect estimate. PSM works in two steps: first, estimate each unit's probability of receiving the treatment (the propensity score) based on observed covariates using logistic regression or machine learning; second, match each treated unit with one or more untreated units that have similar propensity scores, creating balanced comparison groups. For growth teams, PSM is useful for evaluating features that users self-select into (e.g., premium plans, optional onboarding flows, support contacts) where randomization would be inappropriate or impractical.\n\nThe propensity score e(X) = P(Treatment = 1 | X) is estimated by fitting a logistic regression or more flexible models like gradient boosted trees on the observed covariates X. Matching then pairs treated and control units with similar propensity scores using methods like nearest-neighbor matching (each treated unit is matched to the control unit with the closest propensity score), caliper matching (matches are only accepted within a maximum propensity score distance), or kernel matching (all control units contribute with weights inversely related to their propensity score distance). After matching, covariate balance is assessed: the standardized mean differences between treated and control groups on all covariates should be small (typically less than 0.1). The treatment effect is then estimated as the average difference in outcomes between matched treated and control units. Tools include the R packages MatchIt and WeightIt, Python's causalml and DoWhy libraries, and the Stata teffects commands.\n\nPSM should be used when you have rich covariate data that captures the factors driving treatment selection, and when you believe that conditioning on these covariates removes all confounding (the unconfoundedness or selection on observables assumption). This assumption is strong and untestable: if there are unobserved factors that affect both treatment selection and the outcome, PSM will still produce biased estimates. Common pitfalls include including post-treatment variables as covariates (which introduces bias), achieving poor covariate balance after matching (indicating the matching failed), discarding too many treated units that cannot find good matches, and placing excessive trust in the results without sensitivity analysis for unobserved confounding. Alternatives and complements include inverse probability weighting (using propensity scores as weights rather than for matching), doubly robust estimation (combining outcome modeling with propensity scoring), and instrumental variables (which handle unobserved confounding but require a valid instrument).\n\nAdvanced PSM techniques include generalized propensity scores for continuous or multi-valued treatments, coarsened exact matching (CEM) that matches on discretized covariates without estimating a propensity model, and entropy balancing that directly solves for weights that achieve exact covariate balance on specified moments. Sensitivity analysis methods like Rosenbaum bounds quantify how strong unobserved confounding would need to be to overturn the estimated treatment effect, providing a principled way to assess the robustness of PSM findings. For digital experimentation, PSM is increasingly combined with machine learning: causal forests and meta-learners can estimate heterogeneous treatment effects from observational data using propensity scores as inputs, and doubly robust machine learning methods provide valid inference even when either the propensity model or the outcome model is misspecified.",
    category: "experimentation",
    relatedTerms: ["causal-forest", "heterogeneous-treatment-effects", "difference-in-differences"],
    relatedPosts: [],
  },
  {
    slug: "causal-forest",
    term: "Causal Forest",
    definition:
      "A machine learning method based on random forests that estimates heterogeneous treatment effects, discovering how the impact of a treatment varies across different subgroups of users defined by their observable characteristics.",
    explanation:
      "Causal forests extend the random forest algorithm to estimate conditional average treatment effects (CATE): how the effect of a treatment varies as a function of user characteristics. While a standard A/B test provides a single average treatment effect, causal forests estimate a personalized treatment effect for each user based on their features. For growth teams, causal forests unlock the ability to move from one-size-fits-all product experiences to targeted interventions. Instead of shipping a change to all users because it showed a positive average effect, teams can identify which user segments benefit most, which are unaffected, and which might be harmed, enabling more nuanced rollout strategies and personalized experiences.\n\nThe causal forest algorithm, developed by Athey, Tibshirani, and Wager, works by modifying the random forest splitting criterion to maximize the difference in treatment effects across the split rather than the prediction accuracy. Each tree partitions the covariate space into leaves, and within each leaf, the treatment effect is estimated as the difference in average outcomes between treated and control units. The forest aggregates estimates across many trees, and the theoretical framework provides valid confidence intervals for the estimated treatment effects through a form of infinitesimal jackknife. The key insight is using honest estimation: one subsample is used to determine the tree structure (the splits) and a separate subsample is used to estimate the treatment effects within each leaf, preventing overfitting. The grf (generalized random forests) R package and EconML Python library implement causal forests with rigorous statistical guarantees. The inputs are the treatment assignment, the outcome, and a matrix of user features. The output is an estimated treatment effect for each observation.\n\nCausal forests should be used when there is a reasonable expectation that treatment effects vary across users and when the experimental sample is large enough to estimate heterogeneous effects (typically thousands to tens of thousands of observations). The primary use cases include identifying which user segments to target with a treatment, informing personalization strategies, and understanding the mechanisms behind an average treatment effect. Common pitfalls include overstating the reliability of subgroup effects (even with honest estimation, extreme subgroups may have large confidence intervals), using causal forests to mine for significant subgroups without proper correction for multiple comparisons, and confusing prediction of treatment effects with prediction of outcomes. Teams should validate causal forest findings with holdout experiments: if the forest predicts that segment A benefits most, run a targeted A/B test on segment A to confirm.\n\nAdvanced applications include using causal forests to design optimal treatment assignment policies (assign each user to the treatment with the highest estimated CATE), combining causal forests with doubly robust estimation for observational data where treatment is not randomly assigned, and extending to multi-treatment settings where the goal is to choose among several interventions. Meta-learners like the T-learner, S-learner, X-learner, and R-learner provide alternative approaches to heterogeneous treatment effect estimation with different bias-variance tradeoffs. The CATE estimates from causal forests can feed directly into personalization engines, recommendation systems, and targeting algorithms. At companies like Netflix, Spotify, and Stitch Fix, heterogeneous treatment effect estimation informs personalized product experiences where different users receive different variants based on predicted treatment effects rather than a single shipped version.",
    category: "experimentation",
    relatedTerms: ["heterogeneous-treatment-effects", "propensity-score-matching", "contextual-bandit-experiment"],
    relatedPosts: [],
  },
  {
    slug: "heterogeneous-treatment-effects",
    term: "Heterogeneous Treatment Effects",
    definition:
      "Variation in treatment effects across different subgroups of the population, where an intervention may have different impacts depending on user characteristics such as tenure, geography, device type, or behavioral patterns.",
    explanation:
      "Heterogeneous treatment effects (HTE) occur when a treatment's impact varies across individuals or subgroups rather than being uniform. An average treatment effect of +2% conversion lift might mask a +8% lift for new users and a -1% effect for power users. Understanding HTE is crucial for growth teams because it reveals who benefits from a change, who is harmed, and who is unaffected, enabling more targeted and effective product decisions. A feature that shows a modest average improvement might be transformative for a specific segment, and knowing this allows teams to ship changes selectively or to design personalized experiences that serve each segment optimally.\n\nAnalyzing HTE typically follows a progression from simple to sophisticated methods. The simplest approach is pre-specified subgroup analysis: before the experiment, define subgroups of interest (e.g., new vs. returning users, mobile vs. desktop) and estimate the treatment effect within each subgroup. The interaction test formally assesses whether the treatment effect differs significantly across subgroups using an interaction term in the regression model: Y = beta_0 + beta_1*Treatment + beta_2*Subgroup + beta_3*(Treatment*Subgroup) + epsilon, where beta_3 is the differential treatment effect. For discovery-oriented HTE analysis, machine learning methods like causal forests, Bayesian Additive Regression Trees (BART), and meta-learners (T-learner, X-learner, R-learner) estimate personalized treatment effects as a function of many covariates simultaneously. These methods are implemented in the R grf package, Python's EconML and CausalML libraries. Visualization of HTE often uses sorted treatment effect plots where users are ordered by estimated CATE and the cumulative effect is plotted, revealing what fraction of users benefit from the treatment.\n\nHTE analysis should be conducted for every major experiment, but teams must be disciplined about distinguishing confirmatory and exploratory analysis. Pre-specified subgroups with hypothesis-driven rationale (e.g., we expect the new onboarding flow to help new users more than existing users) provide stronger evidence than post-hoc data mining across many possible subgroups. Multiple comparison corrections should be applied when testing many subgroups. Common pitfalls include the garden of forking paths, where analysts search across many subgroup definitions until finding a significant interaction, overfitting in small subgroups where random variation creates the appearance of large effects, and ignoring the fact that average effects in subgroups are still averages that may themselves be heterogeneous. The clinical trials literature has extensive guidance on proper subgroup analysis that digital experimentation teams can adapt.\n\nAdvanced HTE considerations include using the estimated heterogeneity to design optimal treatment policies (which users should receive the treatment and which should not), applying HTE estimates to power analysis for follow-up experiments targeting specific subgroups, and using HTE to understand the mechanisms behind treatment effects. If a treatment works only for users with a specific behavioral pattern, that suggests a mechanistic explanation that can inform future interventions. The concept of the best linear projection of CATE onto user features provides an interpretable summary of the most important dimensions of heterogeneity. For organizations with multiple experiments, meta-analytic approaches can identify consistent patterns of heterogeneity across experiments, revealing general principles like certain user segments are consistently more responsive to UI changes, which informs both experiment design and product strategy.",
    category: "experimentation",
    relatedTerms: ["causal-forest", "propensity-score-matching", "triggered-analysis"],
    relatedPosts: [],
  },
  {
    slug: "intention-to-treat",
    term: "Intention-to-Treat",
    definition:
      "An analysis principle that evaluates experiment results based on the original random assignment of users to treatment groups, regardless of whether they actually received or engaged with the treatment, preserving the validity of randomization.",
    explanation:
      "Intention-to-treat (ITT) analysis is the gold standard analysis approach for randomized experiments. It compares outcomes based on which group users were assigned to, not which treatment they actually received. If a user was assigned to the new onboarding flow but never logged in during the experiment period, they are still counted in the treatment group. If a user assigned to control accidentally received the treatment due to a bug, they remain in the control group. For growth teams, ITT analysis preserves the causal validity of the experiment by maintaining the randomized group composition. Any departure from analyzing by assignment, such as excluding users who did not engage or switching users between groups based on their actual experience, can introduce selection bias that invalidates the experiment.\n\nThe ITT estimator is simply the difference in average outcomes between the assigned treatment group and the assigned control group: tau_ITT = E[Y | Assigned to Treatment] - E[Y | Assigned to Control]. This is an unbiased estimate of the effect of being assigned to treatment, which in the presence of non-compliance (users not receiving their assigned treatment) will generally be a diluted version of the effect of actually receiving the treatment. The relationship between ITT and the per-protocol effect is formalized in the complier average causal effect (CACE or LATE), estimated using instrumental variables: CACE = ITT / compliance_rate. If 80% of users assigned to treatment actually received it, the CACE is the ITT divided by 0.8, which represents the undiluted effect for users who complied with their assignment. This adjustment is valid under the exclusion restriction (assignment affects outcomes only through treatment receipt) and monotonicity (no one does the opposite of their assignment) assumptions.\n\nITT should be the primary analysis for every randomized experiment. It provides a conservative, unbiased estimate of the policy-relevant question: what happens when you roll out this change? Because not all users will engage with any feature, the ITT reflects the realistic impact on the entire eligible population, not just the subset that engages. Common pitfalls include inappropriately excluding non-compliers to inflate the apparent effect size, conditioning on post-randomization variables (like feature usage) which destroys randomization, and confusing ITT with per-protocol estimates in reporting. Teams should report the ITT as the primary result and, if desired, the CACE as a secondary analysis with clear caveats about the additional assumptions required.\n\nAdvanced considerations include principal stratification, which extends the CACE framework to analyze treatment effects for different compliance types (always-takers, never-takers, compliers, defiers) when the treatment is not a simple binary. For experiments where the treatment is an algorithm change that affects different users to different degrees, the ITT can be augmented with triggered analysis that restricts to users who were actually exposed to the differing algorithm behavior, while maintaining ITT as the primary analysis. In multi-sided marketplace experiments, ITT becomes more complex because a user's assignment may affect other users' outcomes through interference. Some organizations adopt modified ITT approaches that handle common practical issues like excluding users who were randomized but never activated the app, though these modifications should be pre-specified and justified.",
    category: "experimentation",
    relatedTerms: ["per-protocol-analysis", "triggered-analysis", "split-testing"],
    relatedPosts: [],
  },
  {
    slug: "per-protocol-analysis",
    term: "Per-Protocol Analysis",
    definition:
      "An analysis approach that evaluates experiment results based on which treatment users actually received rather than their original random assignment, providing an estimate of the treatment effect among compliant users but potentially introducing selection bias.",
    explanation:
      "Per-protocol (PP) analysis restricts the comparison to users who actually received the treatment as intended by their assignment. Users who were assigned to treatment but did not engage with it, or who were assigned to control but somehow received the treatment, are either excluded or reassigned. This approach answers a different question than intention-to-treat: while ITT asks what is the effect of assigning users to this treatment, PP asks what is the effect of actually receiving the treatment. For growth teams, PP analysis is tempting because it estimates the undiluted treatment effect, which is what product managers often want to know. However, PP analysis is susceptible to selection bias because the users who comply with their assignment may differ systematically from those who do not.\n\nPer-protocol analysis is computed by restricting the sample to compliers and comparing outcomes: tau_PP = E[Y | Received Treatment, Assigned Treatment] - E[Y | Received Control, Assigned Control]. The problem is that compliance is a post-randomization outcome that may be correlated with the treatment effect itself. Users who actively engage with a new feature may be more motivated, tech-savvy, or have higher baseline engagement, all of which correlate with better outcomes regardless of the treatment. This means PP analysis confounds the treatment effect with selection effects, potentially making the treatment appear more effective than it truly is. The bias can go in either direction: if sicker patients drop out of treatment, PP analysis in a clinical trial overestimates efficacy; if more engaged users are more likely to encounter a feature change, PP analysis in a digital experiment overestimates the impact.\n\nPer-protocol analysis may be reported as a secondary analysis alongside the primary ITT analysis when teams want to understand the magnitude of the treatment effect for engaged users. It is particularly relevant when non-compliance rates are high and the ITT estimate is severely diluted, making it difficult to assess whether the treatment itself is effective. Common pitfalls include reporting only the PP estimate without the ITT (which overstates impact and is statistically invalid as the primary analysis), not checking whether compliers and non-compliers differ on observable characteristics (which would indicate selection bias), and using PP as justification for shipping when the ITT is not significant. A better alternative to PP analysis is the instrumental variables approach to estimate the complier average causal effect (CACE), which provides the treatment effect for compliers without the selection bias of PP, under certain assumptions.\n\nAdvanced alternatives to simple PP analysis include inverse probability of compliance weighting, which re-weights compliers to represent the full population, and principal stratification, which formally models the types of users (always-compliers, never-compliers, etc.) and estimates treatment effects within each stratum. For digital experiments where exposure is often partial and graded rather than binary, dose-response models can relate the intensity of treatment exposure to the outcome while using random assignment as an instrument to handle selection. The clinical trials literature has extensively debated ITT vs. PP, and the consensus strongly favors ITT as the primary analysis with PP as supplementary. Digital experimentation teams should follow this convention and reserve PP analysis for exploratory investigation rather than primary decision-making.",
    category: "experimentation",
    relatedTerms: ["intention-to-treat", "triggered-analysis", "split-testing"],
    relatedPosts: [],
  },
  {
    slug: "crossover-design",
    term: "Crossover Design",
    definition:
      "An experimental design where the same subjects receive both the treatment and control conditions in different time periods, with each subject serving as their own control, reducing variance from between-subject differences.",
    explanation:
      "In a crossover design, users are exposed to both experimental conditions in sequence. A simple two-period crossover randomly assigns users to two groups: Group 1 receives treatment A then treatment B, while Group 2 receives treatment B then treatment A. By comparing each user's outcomes under both conditions, between-subject variability is eliminated from the treatment effect estimate, dramatically increasing statistical power. For growth teams, crossover designs are particularly valuable when user-to-user variability is high relative to the expected treatment effect, which is common in digital experimentation where user behavior varies widely. The within-subject comparison can reduce variance by 50-80% compared to a between-subject design, enabling smaller sample sizes or shorter experiment durations.\n\nThe analysis of a crossover design accounts for period effects (outcomes may differ between the first and second period regardless of treatment) and sequence effects (the order of treatments may matter). The standard model is: Y_ijk = mu + pi_j + tau_k + lambda_{j-1} + s_i + epsilon_ijk, where pi_j is the period effect, tau_k is the treatment effect, lambda is the carryover effect from the previous treatment, s_i is the subject random effect, and epsilon is the residual error. The treatment effect is typically estimated using a paired analysis: for each subject, compute the difference in outcomes between the two treatment conditions, then test whether this difference's mean is zero. The key advantage is that the subject random effect s_i cancels in the within-subject comparison, removing what is often the largest source of variance. Carryover effects are tested by examining whether the sum of outcomes across periods differs between sequence groups. If carryover is present, only first-period data should be used, sacrificing the within-subject advantage.\n\nCrossover designs should be used when the treatment effect is expected to be temporary and reversible, when there is a sufficient washout period between conditions to prevent carryover, and when user-level variability is a dominant source of noise. In digital experimentation, crossover designs work well for testing UI changes, recommendation algorithm variants, or search ranking modifications where the effect on user behavior does not persist after the treatment is removed. Common pitfalls include carryover effects where the first treatment influences behavior during the second treatment period (a user who learns a new navigation pattern may retain that knowledge even when switched back), dropout during the second period (creating missing data that complicates analysis), and period-by-treatment interactions where the treatment effect genuinely differs between periods.\n\nAdvanced crossover designs include higher-order crossovers with more than two periods (e.g., ABB/BAA designs that allow estimation and testing of carryover), Latin square crossover designs that balance multiple treatments across periods and subjects, and modified crossover designs that incorporate washout periods between treatment conditions to mitigate carryover. For digital experimentation, N-of-1 trials (single-subject crossovers with multiple alternation periods) can provide personalized treatment effect estimates, supporting user-level personalization. Bayesian crossover analysis naturally handles the hierarchical structure of subjects within sequences and periods within subjects, providing individual-level treatment effect estimates with proper uncertainty quantification. The switchback testing design is closely related to crossover but operates at the market or system level rather than the individual user level.",
    category: "experimentation",
    relatedTerms: ["switchback-testing", "latin-square-design", "factorial-design"],
    relatedPosts: [],
  },
  {
    slug: "latin-square-design",
    term: "Latin Square Design",
    definition:
      "An experimental design that controls for two known sources of variation by arranging treatments in a grid where each treatment appears exactly once in each row and column, efficiently balancing nuisance factors without requiring a full factorial experiment.",
    explanation:
      "A Latin square design simultaneously controls for two blocking variables while testing multiple treatments. In a k x k Latin square, k treatments are assigned to a grid of k rows and k columns such that each treatment appears exactly once in each row and once in each column. For example, testing three recommendation algorithms across three time periods and three user segments produces a 3x3 grid where each algorithm is tested in each time period and each segment exactly once, using only 9 cells instead of the 27 required for a full factorial. For growth teams, Latin square designs are efficient for testing multiple treatments when two major sources of variation (like day of week and user segment, or geographic region and time period) need to be controlled without the traffic requirements of a full factorial design.\n\nThe Latin square model decomposes the outcome as: Y_ijk = mu + alpha_i + beta_j + tau_k + epsilon_ijk, where alpha_i is the row effect (e.g., time period), beta_j is the column effect (e.g., user segment), and tau_k is the treatment effect. The design achieves the same precision for the treatment effect estimate as a full factorial would, but with far fewer experimental cells. The analysis uses standard ANOVA with separate F-tests for the row, column, and treatment effects. The key limitation is that the Latin square assumes no interactions between the row factor, column factor, and treatment. If the treatment effect varies across time periods or user segments, the Latin square model does not capture this, and the treatment effect estimate represents an average across all row-column combinations. Replicated Latin squares (using multiple independent squares) or Graeco-Latin squares (which add a third blocking factor) provide additional flexibility.\n\nLatin square designs should be used when there are exactly as many treatments as levels of each blocking factor, when the blocking factors are known to be important sources of variation, and when the assumption of no interactions is reasonable. In digital experimentation, Latin squares are useful for testing multiple UI variants across weekdays and user segments, testing pricing tiers across geographic regions and time periods, or testing notification strategies across user engagement levels and times of day. Common pitfalls include the restrictive requirement that the number of treatments must equal the number of levels of each blocking factor (though incomplete Latin squares can partially relax this), the assumption of no row-by-treatment or column-by-treatment interactions, and the limited residual degrees of freedom for error estimation in small designs.\n\nAdvanced Latin square concepts include Graeco-Latin squares, which superimpose two orthogonal Latin squares to control for three blocking factors simultaneously. When multiple factors need to be tested but the Latin square constraint is too restrictive, incomplete block designs or alpha designs provide more flexibility. For online experimentation, the Latin square principle is often applied informally: ensuring that experiment assignment is balanced across key dimensions like platform, geography, and time. Formal Latin square designs are more common in industrial experimentation and agricultural research, but their principles of balanced blocking can improve the efficiency of digital experiments when adapted to the online setting, particularly for marketplace experiments where both temporal and geographic variation are important nuisance factors.",
    category: "experimentation",
    relatedTerms: ["factorial-design", "crossover-design", "cluster-randomization"],
    relatedPosts: [],
  },
  {
    slug: "cluster-randomization",
    term: "Cluster Randomization",
    definition:
      "An experimental design that randomly assigns groups (clusters) of users rather than individual users to treatment conditions, used when individual randomization is not feasible or when interference between users within the same cluster would violate independence assumptions.",
    explanation:
      "Cluster randomization assigns entire groups of related users to the same treatment condition. Clusters can be geographic regions, companies, schools, social network communities, or any grouping where users interact with each other. For example, randomizing entire companies to test a collaboration feature ensures that all users within a company have the same experience, preventing the confusion of some team members having a feature that others lack. For growth teams, cluster randomization is essential for features that involve user-to-user interactions (messaging, sharing, collaboration), marketplace dynamics (driver-rider matching), or network effects, where individual randomization would create interference between treatment and control users within the same cluster.\n\nThe key statistical consideration in cluster randomization is that the effective sample size is much smaller than the total number of users because users within a cluster are correlated. The design effect, which quantifies this reduction, is DE = 1 + (m - 1) * ICC, where m is the average cluster size and ICC is the intracluster correlation coefficient (the proportion of total variance attributable to between-cluster differences). With an ICC of 0.05 and clusters of 100 users, the design effect is 5.95, meaning you need roughly 6 times as many users as an individually randomized experiment for the same power. The analysis must account for clustering using methods like generalized estimating equations (GEE), mixed-effects models, or cluster-robust standard errors. The simplest approach aggregates outcomes to the cluster level and performs the analysis on cluster-level means, treating clusters as the independent unit. Power analysis for cluster randomized experiments requires specifying both the number of clusters and the cluster size, using formulas that incorporate the ICC: n_clusters = 2*(Z_alpha/2 + Z_beta)^2 * (sigma_b^2 + sigma_w^2/m) / delta^2.\n\nCluster randomization should be used when interference between users makes individual randomization invalid, when the treatment must be applied at the group level for operational reasons, or when contamination between groups would be unacceptable. Common pitfalls include underestimating the sample size requirement by ignoring the design effect, having too few clusters for reliable inference (at least 20-30 clusters per arm is recommended for cluster-robust standard errors), and not measuring or accounting for cluster-level confounders. The loss of power from clustering can be partially offset by stratifying the randomization on important cluster-level characteristics, ensuring balance between treatment and control clusters. Teams should also consider whether partial clustering (some outcomes are clustered, others are individual-level) applies to their setting.\n\nAdvanced cluster randomization designs include stepped-wedge designs, where all clusters eventually receive the treatment but the timing is randomized, providing additional temporal comparisons. Adaptive cluster randomization uses covariate-adaptive methods to improve balance when the number of clusters is small. For social network experiments, graph cluster randomization partitions the social graph into dense communities and randomizes at the community level, reducing interference while preserving more of the individual-level variation than geographic clustering. Platforms like Planout (from Meta) and internal tools at LinkedIn and Uber support cluster-randomized experiments with built-in variance estimation. The increasing importance of network effects in digital products means cluster randomization will only become more prevalent as traditional user-level A/B testing proves inadequate for evaluating social, collaborative, and marketplace features.",
    category: "experimentation",
    relatedTerms: ["switchback-testing", "randomization-unit", "network-effect-experiment"],
    relatedPosts: [],
  },
  {
    slug: "cuped-variance-reduction",
    term: "CUPED Variance Reduction",
    definition:
      "A statistical technique (Controlled-experiment Using Pre-Experiment Data) that reduces metric variance in online experiments by adjusting for pre-experiment user behavior, increasing statistical power by 20-50% without requiring larger sample sizes.",
    explanation:
      "CUPED, introduced by Microsoft Research, is a variance reduction technique that leverages pre-experiment data to increase the precision of experiment estimates. The core idea is that much of the variance in user behavior during an experiment is predictable from the user's behavior before the experiment. By adjusting the outcome metric using a pre-experiment covariate, the residual variance is reduced, and the treatment effect can be detected with fewer users or in less time. For growth teams, CUPED is one of the highest-impact methodological improvements available because it effectively multiplies the statistical power of every experiment without requiring any additional traffic, enabling faster experimentation cycles and the ability to detect smaller effects.\n\nThe CUPED adjustment works as follows: for each user, compute a pre-experiment covariate X (typically the same metric measured in a pre-experiment window, e.g., last week's page views). The adjusted outcome is Y_cuped = Y - theta * (X - X_bar), where theta = Cov(Y, X) / Var(X) is the coefficient that minimizes the variance of the adjusted outcome. The treatment effect is then estimated using Y_cuped instead of Y. Because X was measured before randomization, it is independent of treatment assignment, so the adjustment does not introduce bias. The variance reduction factor is 1 - rho^2, where rho is the correlation between Y and X. If the pre-experiment metric correlates at rho = 0.7 with the experiment metric, CUPED reduces variance by 51%, effectively doubling the sample size. The technique extends naturally to multiple covariates using multivariate regression adjustment: Y_cuped = Y - X * theta, where X is a vector of pre-experiment covariates and theta is the OLS coefficient vector. Modern implementations at companies like Airbnb, Netflix, and Uber use machine learning models to predict user outcomes from rich pre-experiment features, achieving even larger variance reductions.\n\nCUPED should be applied to every experiment where pre-experiment data is available, which is nearly always the case for logged-in users in digital products. The primary requirement is a pre-experiment covariate that is correlated with the outcome metric and measured before randomization. Using the same metric from a prior period (e.g., last week's purchases to adjust this week's purchases) is the simplest and often most effective approach. Common pitfalls include using covariates measured during or after randomization (which introduces bias), not accounting for users without pre-experiment data (new users need separate handling), and applying CUPED to ratio metrics without proper delta method adjustments. The technique is sometimes confused with ANCOVA (analysis of covariance), which is equivalent in the two-group case but CUPED specifically emphasizes the use of pre-experiment rather than baseline covariates.\n\nAdvanced variance reduction techniques extend CUPED in several directions. CUPAC (Controlled-experiment Using Predictions as Covariates), used at Airbnb, trains a machine learning model on pre-experiment data to predict user outcomes, then uses these predictions as the covariate, often achieving larger variance reductions than simple CUPED because the model captures nonlinear relationships. Stratified CUPED applies the adjustment within strata to handle heterogeneous correlation structures. For sequential experiments, the CUPED adjustment must be applied carefully to maintain the validity of sequential stopping boundaries. Some platforms like Statsig and Eppo implement CUPED automatically, applying the pre-experiment adjustment to all metrics by default. The technique has become standard practice at top experimentation organizations and represents one of the few free lunches in statistics: more precise estimates with no additional data collection cost.",
    category: "experimentation",
    relatedTerms: ["power-analysis", "split-testing", "stratified-randomization"],
    relatedPosts: [],
  },
  {
    slug: "triggered-analysis",
    term: "Triggered Analysis",
    definition:
      "An analysis technique that restricts experiment evaluation to users who actually encountered or were exposed to the experimental change, reducing noise from unaffected users while maintaining the validity of the randomization through careful implementation.",
    explanation:
      "Triggered analysis improves experiment sensitivity by focusing on the subset of users who actually experienced the difference between treatment and control. In many experiments, a large fraction of assigned users never encounter the changed feature during the experiment window. For example, if an experiment changes the checkout flow, users who never reach checkout during the experiment period add noise without contributing signal. Triggered analysis restricts the comparison to users who were triggered, meaning they reached the point in the product where treatment and control diverge. For growth teams, triggered analysis can dramatically increase the effective power of experiments, often reducing required sample sizes by 50-80%, enabling faster decisions and the ability to detect smaller effects.\n\nThe implementation of triggered analysis requires that the triggering event (the point where users would experience the change) is logged identically for both treatment and control users. This is critical: the trigger must be defined based on reaching a point in the user flow, not on seeing the treatment. If the trigger is only logged for treatment users who see the new UI, but not for control users who see the old UI at the same point, the analysis is invalid. The correct approach logs the trigger for all users who reach the relevant product surface, regardless of their assignment. The analysis then compares outcomes between triggered treatment users and triggered control users. Because both groups were randomly assigned and the trigger condition is the same for both, the comparison preserves the causal validity of the original randomization. The variance reduction comes from eliminating the dilution caused by users who were never exposed to any difference.\n\nTriggered analysis should be used whenever a significant fraction of randomized users never encounter the experimental change. It is particularly valuable for experiments on features deep in the user funnel (checkout changes, settings page modifications, advanced feature updates) where only a minority of users navigate to the affected surface during the experiment. Common pitfalls include defining the trigger based on treatment-specific behavior (which creates selection bias), not logging trigger events for the control group, and not recognizing that triggered analysis changes the estimand from the full-population ITT effect to the treatment effect among the triggered subpopulation. Teams should always report both the full-population ITT and the triggered analysis results, with clear labels distinguishing them. The triggered effect will be larger than the ITT effect because it is not diluted by unaffected users.\n\nAdvanced triggered analysis techniques include dilution-adjusted triggered analysis, which multiplies the triggered effect by the triggering rate to recover the full-population ITT estimate with reduced variance. This approach, described in papers from Microsoft Research and others, provides the best of both worlds: the precision of triggered analysis with the interpretability of the full-population effect. For experiments with continuous exposure intensity (where some users are heavily exposed and others barely), inverse propensity of exposure weighting can adjust for differential exposure while maintaining the causal interpretation. Some experimentation platforms implement triggered analysis automatically when trigger events are defined in the experiment configuration. The concept extends to heterogeneous triggering: different user segments may have different triggering rates, and the triggered treatment effect may vary across these segments, providing additional insight into who benefits from the change and why.",
    category: "experimentation",
    relatedTerms: ["intention-to-treat", "per-protocol-analysis", "cuped-variance-reduction"],
    relatedPosts: [],
  },
  {
    slug: "growth-experimentation-framework",
    term: "Growth Experimentation Framework",
    definition:
      "A structured organizational process for systematically generating, prioritizing, running, and learning from experiments across the entire user lifecycle, designed to maximize the rate of validated learning and compound the impact of product improvements.",
    explanation:
      "A growth experimentation framework provides the organizational scaffolding that transforms ad hoc testing into a systematic, scalable practice. It encompasses the full lifecycle from hypothesis generation through experiment design, execution, analysis, and knowledge sharing. The framework typically includes an idea backlog with standardized hypothesis templates, a prioritization system (often ICE: Impact, Confidence, Ease), experiment design templates that enforce statistical rigor, a review process to catch common errors before launch, automated analysis pipelines, and a knowledge repository that captures learnings from every experiment. For growth teams, having a formal framework is the difference between occasional testing and a compounding experimentation engine that generates sustained product improvement.\n\nThe core components of a growth experimentation framework include: (1) Ideation and hypothesis generation, using frameworks like the growth model to identify high-leverage areas and formulating hypotheses in the format: we believe that [change] will cause [effect] for [audience] because [rationale]. (2) Prioritization, scoring experiments on expected impact (how much will the metric move), confidence (how sure are we in the hypothesis), and ease (how quickly can we build and analyze it). (3) Experiment design, including power analysis, metric selection (primary, secondary, guardrail), randomization unit choice, and analysis plan documentation. (4) Execution, using platforms like Statsig, Optimizely, LaunchDarkly, or Eppo to manage traffic allocation, feature flagging, and data collection. (5) Analysis, with standardized statistical methods, sequential testing, and automated reporting. (6) Documentation and knowledge sharing, ensuring that every experiment's results, including null results, are recorded and accessible to the organization.\n\nTeams should implement a growth experimentation framework when they are running more than a few experiments per quarter and want to scale their experimentation practice. The framework should be lightweight enough not to slow down experimentation but rigorous enough to prevent common errors. Common pitfalls include over-engineering the framework with so much process that experiment velocity drops, under-investing in the knowledge management component so that the same hypotheses are tested repeatedly, not including guardrail metrics that catch negative side effects, and allowing the framework to become a gate that prevents junior team members from running experiments. The most successful frameworks balance rigor with accessibility, providing templates and tools that make it easy to do the right thing rather than relying on individual expertise.\n\nAdvanced framework elements include automated experiment sizing and duration estimation, experiment interaction detection that warns when concurrent experiments might interfere with each other, Bayesian meta-analysis that aggregates learnings across experiments to update organizational priors, and experiment portfolio management that balances exploitation (optimizing known levers) with exploration (testing novel hypotheses). Some organizations implement experimentation maturity models that progress from ad hoc testing through standardized processes to fully automated, AI-assisted experimentation. The most mature organizations treat their experiment catalog as a strategic asset, using machine learning to identify patterns across hundreds of past experiments and predict which future hypotheses are most likely to succeed.",
    category: "experimentation",
    relatedTerms: ["experiment-velocity", "experiment-review-board", "experiment-documentation"],
    relatedPosts: [],
  },
  {
    slug: "activation-experiment",
    term: "Activation Experiment",
    definition:
      "An experiment specifically designed to increase the rate at which new users reach a product's activation milestone, the key early action that correlates with long-term retention, by testing changes to onboarding flows, first-run experiences, and value delivery.",
    explanation:
      "Activation experiments focus on the critical transition from sign-up to first value realization, testing changes that help new users reach the aha moment faster and more reliably. The activation metric varies by product: for a project management tool it might be creating a first project with at least two tasks, for a social app it might be connecting with five friends, for an analytics platform it might be creating a first dashboard. For growth teams, activation is often the highest-leverage stage of the funnel because improvements compound: a user who activates is dramatically more likely to retain, expand, and refer others. Research from companies like Slack, Dropbox, and Pinterest has shown that identifying and optimizing toward the right activation metric can be the single most impactful growth initiative.\n\nActivation experiments typically test changes in several categories: reducing friction in the setup process (eliminating unnecessary steps, pre-filling defaults, offering templates), guiding users toward the activation milestone (progress indicators, contextual prompts, interactive tutorials), demonstrating value early (pre-populated sample data, showing what the product looks like when fully configured), and leveraging social proof or personalization to motivate completion. The experiment design should use the activation milestone as the primary metric, with time-to-activation as a secondary metric and long-term retention (7-day, 30-day) as a validation metric. Analysis should track the full activation funnel to understand where in the process the treatment had its effect. Platforms like Statsig and Amplitude support funnel-based experiment analysis that shows how treatments affect each step in the activation sequence.\n\nActivation experiments should be prioritized when the analysis of existing data shows a significant drop-off between sign-up and activation, when the activation rate varies significantly across user segments (indicating room for improvement in underperforming segments), or when the product's activation metric has been validated as a leading indicator of long-term retention. Common pitfalls include optimizing for a vanity activation metric that does not actually correlate with retention, over-simplifying the activation flow to the point where users activate but do not understand the product, and not segmenting activation rates by acquisition channel (users from different sources may need different activation experiences). Teams should validate their activation metric by confirming that it predicts retention in a causal sense, not just a correlational one.\n\nAdvanced activation experimentation includes using machine learning to personalize the onboarding experience based on user characteristics detected at sign-up (industry, role, company size, acquisition source), implementing adaptive onboarding that adjusts based on user behavior during the flow, and testing activation interventions that extend beyond the product itself (welcome emails, push notification sequences, human outreach for high-value accounts). Multi-touch activation experiments test combinations of interventions across channels, recognizing that activation is often the result of multiple interactions rather than a single in-product moment. For enterprise products, activation experiments may target team-level or organization-level metrics rather than individual user metrics, recognizing that collaborative products activate when teams adopt, not just when individuals sign up.",
    category: "experimentation",
    relatedTerms: ["onboarding-experiment", "retention-experiment", "growth-experimentation-framework"],
    relatedPosts: [],
  },
  {
    slug: "retention-experiment",
    term: "Retention Experiment",
    definition:
      "An experiment aimed at increasing the percentage of users who continue using a product over time, testing interventions that strengthen habit formation, increase perceived value, reduce churn triggers, and deepen user engagement.",
    explanation:
      "Retention experiments are among the most valuable and challenging experiments a growth team can run. While acquisition experiments add users to the top of the funnel, retention experiments determine how many stay, making retention the multiplier on all other growth efforts. A 5% improvement in retention can be worth more than a 20% improvement in acquisition because retained users compound value over time through engagement, revenue, and referrals. For growth teams, retention experimentation requires longer time horizons, more nuanced metrics, and deeper understanding of user psychology than most other experiment types. The challenge is that retention is a lagging indicator: the impact of a change on 30-day or 90-day retention takes 30-90 days to measure, creating a fundamental tension with the desire for fast experiment velocity.\n\nRetention experiments span multiple intervention categories. Engagement experiments test features that increase the frequency and depth of product usage: notification strategies, content personalization, recommendation quality, and feature discovery mechanisms. Habit formation experiments focus on creating regular usage patterns through triggers (reminders, summaries, updates), investment (content creation, social connections, customization), and variable rewards. Re-engagement experiments target users who have started to lapse, testing win-back campaigns, dormancy notifications, and product-return incentives. Value deepening experiments test features that expand the user's investment in the product, such as integrations, data import, and collaborative features that create switching costs. The primary metric for retention experiments is typically day-N retention (the percentage of users active on day N after treatment exposure) or bounded retention (active in any N-day window). Proxy metrics like session frequency, feature adoption, and engagement depth provide leading indicators that can shorten the feedback loop.\n\nRetention experiments should be designed with careful attention to the time dimension. The minimum experiment duration should cover at least one full retention cycle (if measuring 30-day retention, the experiment needs at least 30 days of post-enrollment observation). Because retention is measured over time, the analysis must account for the maturation of cohorts: only users enrolled early enough to have completed the observation window should be included in the analysis. Common pitfalls include using short-term engagement proxies that do not actually predict long-term retention, running retention experiments for too short a duration and drawing conclusions from incomplete data, and failing to account for novelty effects that temporarily boost engagement. Teams should also monitor for negative side effects: a notification strategy that improves 7-day retention but annoys users into disabling notifications may harm 90-day retention.\n\nAdvanced retention experimentation leverages survival analysis to model the entire retention curve rather than focusing on a single time point, enabling detection of effects on the shape of the retention function. Causal survival forests can estimate heterogeneous retention effects, identifying which user segments respond most to retention interventions. Long-running holdout groups provide ground truth on the cumulative impact of retention experiments over months or years. For subscription products, retention experiments often focus on cancellation flow interventions (save offers, plan downgrades, pause options) and can use regression discontinuity designs around renewal dates. Machine learning-based churn prediction models can identify at-risk users for targeted retention interventions, with the model predictions feeding into experiment targeting to create personalized retention programs.",
    category: "experimentation",
    relatedTerms: ["activation-experiment", "long-running-experiment", "growth-experimentation-framework"],
    relatedPosts: [],
  },
  {
    slug: "onboarding-experiment",
    term: "Onboarding Experiment",
    definition:
      "An experiment that tests changes to the new user onboarding flow, measuring the impact on activation rates, time-to-value, and early retention by modifying the sequence, content, and mechanics of the initial product experience.",
    explanation:
      "Onboarding experiments target the first minutes to days of a user's product experience, testing how different introductory flows affect whether users understand the product's value and begin using it regularly. The onboarding flow is where first impressions are formed and where the largest absolute drop-offs typically occur: it is common for 40-60% of new sign-ups to never complete onboarding. For growth teams, onboarding is one of the highest-impact experiment areas because improvements directly translate to more activated users entering the retention funnel. Every percentage point of onboarding completion improvement multiplies against all downstream metrics including retention, engagement, and revenue.\n\nOnboarding experiments test a variety of design patterns: progressive disclosure versus comprehensive setup, interactive tutorials versus static walkthroughs, personalized flows based on user characteristics versus one-size-fits-all sequences, required steps versus optional exploration, and different strategies for collecting initial user data. The experiment should track a metrics hierarchy: the primary metric is typically the activation rate or onboarding completion rate, secondary metrics include time-to-activation and specific step-level completion rates, and long-term validation metrics include 7-day and 30-day retention. The experiment design must account for the fact that onboarding is sequential: a change to step 2 can only affect users who completed step 1, so the analysis should consider conditional completion rates at each step. Tools like Appcues, Userpilot, and Pendo allow non-technical teams to build and test onboarding flows without engineering support.\n\nOnboarding experiments should be run continuously since the onboarding flow is where growth teams can most rapidly iterate and learn. The relatively high traffic of new users and the large effect sizes typical in onboarding (individual steps might see 10-30% relative improvements) make onboarding experiments faster to reach significance than most other experiment types. Common pitfalls include over-optimizing for onboarding completion at the expense of understanding (users who speed through onboarding may not actually learn to use the product), not segmenting by acquisition source (users from different channels may need different onboarding experiences), testing too many changes simultaneously without a structured multivariate or factorial design, and ignoring mobile versus desktop differences in onboarding effectiveness.\n\nAdvanced onboarding experimentation includes using machine learning to create adaptive onboarding flows that adjust in real time based on user behavior during the flow, testing personalized onboarding paths based on user self-reported goals or automatically detected characteristics, implementing branching onboarding that routes users through different paths based on their responses and engagement levels, and testing the integration of onboarding with lifecycle email and push notification sequences. Multi-channel onboarding experiments recognize that the onboarding journey extends beyond the product itself to include welcome emails, getting-started guides, community invitations, and human touchpoints for high-value accounts. The most sophisticated approaches use reinforcement learning to continuously optimize the onboarding sequence, treating each step and decision point as an action in a multi-step optimization problem.",
    category: "experimentation",
    relatedTerms: ["activation-experiment", "growth-experimentation-framework", "feature-gating"],
    relatedPosts: [],
  },
  {
    slug: "pricing-experiment",
    term: "Pricing Experiment",
    definition:
      "An experiment that tests different pricing structures, price points, packaging configurations, or billing models to optimize revenue, conversion rates, or a combination of monetization metrics while monitoring the impact on user satisfaction and retention.",
    explanation:
      "Pricing experiments are among the most impactful and most sensitive experiments a growth team can run. Price directly affects willingness to pay, conversion rate, revenue per user, and perceived value. Unlike most product experiments where the downside risk is modest, pricing experiments can have significant financial consequences and raise fairness concerns when different users see different prices. For growth and monetization teams, pricing experimentation is essential because optimal pricing is rarely discovered through intuition alone, it requires systematic testing. Companies like Netflix, Spotify, and Uber have extensive pricing experimentation programs that continuously test pricing structures, tiers, and promotional offers.\n\nPricing experiments can test several dimensions: price level (testing different price points for the same product), packaging (which features are included in each tier), billing frequency (monthly vs. annual), framing (how the price is presented, including anchoring and decoy effects), promotional offers (discounts, free trials, and their duration), and price localization (different prices for different markets). The experimental design must carefully choose the right metrics: revenue per user is the most direct measure, but conversion rate, long-term retention, and lifetime value should also be tracked as secondary and guardrail metrics. A price increase that improves revenue per converting user but reduces conversion rate may or may not be net positive, depending on the elasticity. Analysis should include willingness-to-pay curves estimated from the experimental data, enabling optimization of the full revenue function rather than a single price point.\n\nPricing experiments require extra caution due to ethical and practical considerations. Users who discover they are paying different prices than others may feel cheated, creating trust issues and PR risk. Mitigation strategies include testing prices only for new users (existing users see current pricing), testing at the geographic market level (which is a common legitimate practice), testing packaging and framing rather than raw price, and ensuring any experimental price can be honored long-term. Common pitfalls include not running the experiment long enough to observe the impact on renewal and churn (a price increase may show good initial conversion but higher long-term churn), ignoring the denominator (measuring revenue per visitor rather than per subscriber conflates conversion and pricing effects), and not testing enough price points to map the demand curve. Van Westendorp price sensitivity analysis and Gabor-Granger analysis provide complementary survey-based methods for initial price range identification before experimental testing.\n\nAdvanced pricing experimentation includes conjoint analysis-based experiments that test combinations of features and prices to estimate the relative value users place on different features, dynamic pricing experiments that adjust prices based on demand signals (common in travel, e-commerce, and ridesharing), and machine learning-based price optimization that personalizes pricing based on predicted willingness to pay. Subscription pricing experiments are particularly complex because they involve upfront conversion, ongoing retention, and upgrade/downgrade dynamics that play out over months. Experimentation platforms like Statsig and LaunchDarkly support pricing experiments through feature flag configurations that control pricing display, with careful attention to ensuring that users see consistent pricing across sessions and devices.",
    category: "experimentation",
    relatedTerms: ["paywall-testing", "monetization-experiment", "growth-experimentation-framework"],
    relatedPosts: [],
  },
  {
    slug: "paywall-testing",
    term: "Paywall Testing",
    definition:
      "Experiments that test the design, timing, placement, and configuration of paywall experiences where free users encounter the boundary between free and paid features, optimizing the balance between conversion to paid and engagement retention.",
    explanation:
      "Paywall testing optimizes the critical conversion point where free or trial users encounter the gate between free functionality and premium features. The paywall is one of the highest-leverage conversion surfaces in freemium and subscription products: small changes in paywall design, copy, or placement can produce large revenue improvements. For growth teams, paywall experimentation is essential because the paywall represents the key monetization moment and the most common point of user friction. Getting the paywall wrong, either too aggressive (killing engagement) or too permissive (leaving money on the table), directly impacts the business's revenue trajectory.\n\nPaywall experiments test multiple dimensions: what triggers the paywall (number of uses, specific premium features, time-based limits), where it appears (inline within the content, as a modal, as a separate page), when it appears in the user journey (immediately, after a period of free usage, upon attempting specific actions), what information is displayed (pricing, feature comparison, social proof, urgency elements), and how dismissible it is (hard paywall requiring upgrade vs. soft paywall allowing limited continued free use). The primary metric is typically conversion to paid, but the analysis must also include total engagement, free user retention, and long-term subscriber retention as guardrail metrics. A paywall that increases immediate conversion by creating urgency may damage long-term metrics if it pushes uncommitted users into subscriptions they later cancel. Tools like RevenueCat, Adapty, and Purchasely provide specialized paywall testing infrastructure for mobile apps.\n\nPaywall experiments should be run continuously and should be informed by funnel analysis showing where users encounter the paywall, how many dismiss it, and what happens afterward. The most effective paywall testing programs combine quantitative experiments with qualitative research (user interviews, session recordings of paywall encounters) to understand the emotional and cognitive factors driving the conversion or rejection decision. Common pitfalls include testing only the paywall design without testing the timing and trigger rules, not accounting for the full user lifecycle impact (a tighter paywall may improve short-term conversion but reduce the pool of engaged free users who convert later), and anchoring on the current paywall paradigm rather than testing fundamentally different approaches.\n\nAdvanced paywall experimentation includes adaptive paywalls that adjust based on user behavior signals (users showing high engagement might see a softer paywall to avoid disrupting their experience, while casual users might see a harder paywall), ML-based paywall personalization that predicts user-level conversion probability and adjusts the paywall intensity accordingly, and multi-step paywall sequences that test progressive restriction strategies. A/B testing within the paywall itself (different CTA copy, pricing display, feature lists) can be nested within broader paywall strategy experiments using factorial designs. For content-based products, testing the amount of free content visible before the paywall (hard cutoffs vs. blurred previews vs. full content with upgrade prompts) can have dramatic effects on both SEO value and conversion rates.",
    category: "experimentation",
    relatedTerms: ["pricing-experiment", "monetization-experiment", "feature-gating"],
    relatedPosts: [],
  },
  {
    slug: "monetization-experiment",
    term: "Monetization Experiment",
    definition:
      "An experiment focused on increasing revenue per user through changes to pricing, upsell flows, premium feature presentation, upgrade prompts, and payment mechanics, measuring both immediate revenue impact and long-term customer lifetime value.",
    explanation:
      "Monetization experiments test changes that directly affect how much revenue the product generates from its user base. Unlike acquisition experiments that bring in more users or retention experiments that keep users longer, monetization experiments focus on extracting more value from the existing user base through better pricing, packaging, upselling, and payment experiences. For growth teams, monetization experimentation is critical because it often has the highest ROI: a 10% improvement in revenue per user translates directly to a 10% revenue increase without requiring any additional users, making it one of the most efficient growth levers available.\n\nMonetization experiments span a wide range of interventions: testing premium feature placement and promotion within the product, optimizing upgrade flows and calls to action, experimenting with in-app purchase mechanics and pricing, testing subscription plan structures and feature bundles, optimizing the checkout experience to reduce payment abandonment, testing payment method options and billing frequencies, and experimenting with discounts, coupons, and promotional pricing. The primary metric is typically average revenue per user (ARPU) or revenue per paying user (ARPPU), with secondary metrics including conversion to paid, upsell rate, and purchase frequency. Crucially, guardrail metrics must include user satisfaction scores, support ticket rates, and churn rates to ensure that monetization improvements do not come at the expense of user experience. Revenue metrics should be measured over sufficient time horizons to capture subscription renewal behavior, not just initial conversion.\n\nMonetization experiments should be run when the product has established product-market fit and has a meaningful user base. Premature monetization optimization can damage growth by alienating users before they are fully engaged. Common pitfalls include optimizing for short-term revenue at the expense of long-term lifetime value (aggressive upselling may increase immediate revenue but increase churn), not testing enough of the monetization stack (many teams test only the pricing page when the entire upgrade funnel from feature discovery through payment completion offers optimization opportunities), and failing to segment monetization metrics by user type (enterprise users may respond differently to pricing changes than SMBs or consumers). Teams should also watch for cannibalization effects where promoting one product or tier reduces sales of another.\n\nAdvanced monetization experimentation includes using machine learning to personalize upgrade prompts based on predicted user value and conversion probability, testing dynamic pricing that adjusts based on user engagement signals, implementing sophisticated paywall optimization using contextual bandits that learn the optimal paywall configuration for each user type, and running expansion revenue experiments that test cross-sell and upsell strategies for existing paying customers. For marketplace products, monetization experiments may test take rates, pricing algorithms, and fee structures that affect both sides of the market simultaneously. The interaction between monetization and retention creates complex optimization problems where the optimal price or upsell strategy depends on the retention function, requiring joint experimentation across both dimensions.",
    category: "experimentation",
    relatedTerms: ["pricing-experiment", "paywall-testing", "growth-experimentation-framework"],
    relatedPosts: [],
  },
  {
    slug: "referral-testing",
    term: "Referral Testing",
    definition:
      "Experiments that optimize referral and invitation programs by testing incentive structures, sharing mechanics, referral messaging, and the invitation experience to maximize the number and quality of referred users.",
    explanation:
      "Referral testing focuses on optimizing the viral loop that turns existing users into acquisition channels for new users. Effective referral programs can dramatically reduce customer acquisition costs while bringing in higher-quality users who tend to retain better than paid acquisition channels. For growth teams, referral experimentation is a high-leverage activity because referral programs involve multiple optimization surfaces: the prompt that asks users to refer, the incentive structure for both referrer and invitee, the sharing mechanism and channel, the referral message content, and the landing experience for the referred user. Each surface can be independently tested and optimized.\n\nReferral experiments test several key dimensions: incentive type and magnitude (cash credits, feature unlocks, service upgrades, charitable donations), incentive structure (single-sided rewards vs. double-sided where both referrer and invitee benefit), referral prompt timing and placement (post-purchase, during onboarding, at moments of delight, in settings), sharing channel mechanics (email, SMS, social media, direct link, QR code), referral message content and framing (personalized vs. generic, emphasizing the product vs. the reward), and the invitation landing page experience. The key metrics form a referral funnel: share rate (percentage of users who initiate a referral), invitation rate (number of invitations sent per sharing user), acceptance rate (percentage of invitations that result in sign-ups), and referral quality metrics (activation and retention rates of referred users). The viral coefficient K = share_rate * invitations_per_sharer * acceptance_rate * activation_rate captures the overall referral efficiency.\n\nReferral experiments should be designed with attention to both the quantity and quality of referrals. A referral incentive that increases the share rate dramatically might attract low-quality referrals from users gaming the system for rewards. Common pitfalls include measuring only referral volume without tracking referral quality (retention and monetization of referred users), testing incentive magnitude without testing the framing and presentation of the incentive, ignoring the channel-specific dynamics of different sharing mechanisms (sharing via social media versus email involves very different user behavior and expectations), and not testing the full referral flow end-to-end including the referred user's experience. Teams should also consider the natural referral behavior that occurs without any program and measure incrementality: how many additional referrals does the program generate beyond what would happen organically.\n\nAdvanced referral experimentation includes testing tiered referral programs that offer escalating rewards for multiple referrals, ambassador or advocate programs that give power referrers special tools and recognition, referral gamification elements like leaderboards and challenges, and machine learning-based targeting that identifies users with the highest referral potential and serves them personalized prompts. Network analysis can identify users who are most connected and influential, enabling targeted referral campaigns. For B2B products, referral experiments may test different incentive structures for individual users versus companies, and may involve longer referral cycles that require different measurement approaches. The interaction between referral programs and other growth channels creates attribution challenges that should be addressed in the experiment design.",
    category: "experimentation",
    relatedTerms: ["virality-testing", "growth-experimentation-framework", "activation-experiment"],
    relatedPosts: [],
  },
  {
    slug: "virality-testing",
    term: "Virality Testing",
    definition:
      "Experiments that measure and optimize the organic spread of a product through user actions, testing features and mechanics that naturally encourage sharing, collaboration, and exposure of the product to non-users without explicit referral incentives.",
    explanation:
      "Virality testing focuses on the organic sharing and exposure mechanisms built into the product itself, distinct from referral programs that offer explicit incentives. Viral features cause the product to spread naturally through user actions: sharing content to social media, inviting collaborators to a project, sending documents or links that expose recipients to the product, or creating visible output that drives awareness. For growth teams, virality is the holy grail of growth because it creates self-sustaining user acquisition with near-zero marginal cost. Products like Dropbox (file sharing links), Calendly (scheduling links), Slack (team invitations), and Figma (collaborative design) all grew primarily through viral mechanics embedded in the core product experience.\n\nVirality experiments test changes to the product that affect how naturally and frequently the product is exposed to non-users. This includes testing the visibility and ease of sharing features, the quality of the shared content experience for recipients (does a shared link provide a compelling product preview?), the friction of the conversion funnel for virally exposed users (can they quickly see value without signing up?), and the integration with external platforms where sharing occurs (social media previews, email formatting, messaging app rich previews). The key metrics are the viral coefficient (K-factor) and the viral cycle time. K = i * c, where i is the average number of invitations or exposures per user and c is the conversion rate of those exposures. If K > 1, each user generates more than one new user on average, creating exponential growth. The viral cycle time measures how quickly this loop completes, from a user joining to their contacts being exposed and converting.\n\nVirality experiments should focus on the highest-volume sharing channels and the most natural sharing behaviors in the product. Testing should evaluate both the sending side (how easily and frequently do users share) and the receiving side (how compelling is the experience for recipients and how many convert). Common pitfalls include conflating paid referral mechanics with organic virality (they are fundamentally different growth channels), measuring only sharing volume without tracking the full viral loop through to conversion and activation, and not testing the shared content experience from the recipient's perspective (many products create shared links that provide a poor first impression). Teams should also be aware that viral mechanics are highly sensitive to product-market fit: forcing sharing in a product that does not naturally lend itself to sharing will create user annoyance rather than growth.\n\nAdvanced virality experimentation includes testing content virality mechanics where user-generated content spreads organically across platforms (TikTok and Instagram effects are examples), implementing and testing built-in viral loops like collaborative workspaces that inherently require inviting others, optimizing Open Graph and social sharing previews that determine how shared links appear on social platforms, and testing product-led growth mechanics where free users create publicly visible output that drives awareness. Network effect experiments, which are closely related, test features that make the product more valuable as more users join, creating a pull-based viral dynamic rather than a push-based one. For platforms and marketplaces, virality experiments may test mechanisms that encourage supply-side or demand-side users to invite their counterparts.",
    category: "experimentation",
    relatedTerms: ["referral-testing", "network-effect-experiment", "growth-experimentation-framework"],
    relatedPosts: [],
  },
  {
    slug: "feature-gating",
    term: "Feature Gating",
    definition:
      "The practice of controlling access to product features based on configurable rules, enabling gradual rollouts, targeted access, and experiments by dynamically determining which users see which features without code deployments.",
    explanation:
      "Feature gating, also called feature flagging, is the infrastructure that enables modern experimentation by decoupling feature release from code deployment. A feature gate is a conditional check in the code that determines whether a user should see a feature based on rules that can be changed without redeploying. Gates can be based on user attributes (plan type, geography, cohort), random assignment (for experiments), percentage rollouts (for gradual releases), or arbitrary targeting rules. For growth teams, feature gating is foundational infrastructure: it enables running experiments without engineering bottlenecks, performing safe rollouts that can be instantly rolled back, testing features with specific user segments, and managing the complexity of having multiple simultaneous experiments.\n\nFeature gating systems consist of several components: a management interface where gates are created and configured, a server-side SDK that evaluates gate rules for each user request, a client-side SDK for front-end gating, an event logging system that records which users were exposed to which features, and an analysis layer that connects exposure data to outcome metrics. Leading platforms include LaunchDarkly (specialized feature management), Statsig (feature gating with integrated experimentation and analytics), Split.io, and open-source options like Unleash and Flipt. The implementation pattern is straightforward: if (featureGate.check(user, 'new-checkout-flow')) { showNewCheckout(); } else { showOldCheckout(); }. The gate evaluation happens in real time, checking the user's attributes against the configured rules and returning a boolean or variant assignment.\n\nFeature gating should be implemented as core infrastructure before scaling an experimentation program. Without proper gating, each experiment requires custom code to handle assignment and exposure, creating technical debt and limiting experiment velocity. Common pitfalls include accumulating stale feature gates that are never cleaned up (creating code complexity), not logging gate evaluations consistently (making it impossible to analyze experiment exposure), having inconsistent gate evaluation between server and client (creating flickering experiences), and not implementing proper fallback behavior when the gating service is unavailable. Teams should establish a gate lifecycle process: gates are created for experiments or rollouts, monitored during the experiment period, and then either removed (if the feature is fully shipped or abandoned) or converted to permanent configuration.\n\nAdvanced feature gating includes dynamic configuration (gates that return not just boolean values but configuration parameters like colors, copy, or algorithm weights), mutual exclusion layers (ensuring that users in one experiment are not simultaneously in a conflicting experiment), holdout group management (maintaining persistent groups excluded from all feature changes), and gradual rollout automation that monitors guardrail metrics and automatically pauses a rollout if degradation is detected. Some platforms support feature gate dependencies, where one gate's evaluation depends on another, enabling complex rollout strategies. The trend toward product-led growth has made feature gating a revenue tool as well: gating premium features by plan type, implementing usage-based limits, and managing trial experiences all use the same underlying infrastructure.",
    category: "experimentation",
    relatedTerms: ["percentage-rollout", "split-testing", "guardrail-metric-testing"],
    relatedPosts: [],
  },
  {
    slug: "percentage-rollout",
    term: "Percentage Rollout",
    definition:
      "A deployment strategy that gradually increases the percentage of users who receive a new feature from a small initial percentage to full deployment, monitoring key metrics at each stage to catch problems before they affect the entire user base.",
    explanation:
      "Percentage rollout is a risk management technique that sits between a controlled experiment and a full launch. Instead of shipping a feature to all users at once, the feature is released to a small percentage (e.g., 1-5%), then progressively expanded to 10%, 25%, 50%, and finally 100% as confidence builds that the feature is working correctly and not causing problems. For growth teams, percentage rollouts provide a safety net for changes that have already been validated through experiments but might have implementation issues at scale, or for changes where a full experiment is not feasible but the risk of a big-bang launch is unacceptable. The technique is standard practice at major tech companies and is built into feature management platforms like LaunchDarkly, Statsig, and Harness.\n\nA percentage rollout uses the same hashing mechanism as experiment assignment to ensure consistent user experience: a user who sees the feature at 5% rollout will continue to see it at 10%, 25%, and beyond. At each rollout stage, key metrics are monitored to detect any degradation. These metrics typically include performance indicators (latency, error rates, crash rates), business metrics (conversion rates, revenue), and user experience metrics (support tickets, satisfaction scores). The rollout advances to the next stage only when metrics are confirmed to be healthy. If problems are detected, the rollout can be instantly rolled back to 0%, limiting the blast radius. Some platforms automate this process with automated rollout rules: advance to the next percentage if guardrail metrics remain within acceptable bounds for 24 hours, and automatically roll back if any metric breaches a threshold.\n\nPercentage rollouts should be used for any significant feature launch, even if the feature has already been validated through an A/B test. The test validates the feature's impact on user behavior; the rollout validates the implementation's reliability at scale. Common pitfalls include advancing the rollout too quickly without waiting for sufficient data at each stage, monitoring only technical metrics without tracking business impact, not having automated rollback triggers for critical metrics, and treating the rollout as an experiment without proper statistical controls (a rollout is not a substitute for an experiment because it lacks a persistent control group). Teams should also be aware that a percentage rollout introduces a temporary inconsistency in the user experience, which can be confusing for shared features in collaborative products.\n\nAdvanced rollout strategies include ring-based deployment (deploying first to internal users, then to beta users, then to a random sample, then to all), geography-based rollouts (launching in one market first), canary deployments (where a small percentage of servers run the new code), and automated progressive delivery systems that integrate feature flagging with CI/CD pipelines. Dark launches, where the new code path is executed but its output is not shown to users, can validate technical performance before any user-visible rollout. Some organizations combine percentage rollouts with experiment analysis, creating a monitored rollout that also produces valid causal estimates by maintaining a small persistent control group throughout the rollout process.",
    category: "experimentation",
    relatedTerms: ["feature-gating", "guardrail-metric-testing", "split-testing"],
    relatedPosts: [],
  },
  {
    slug: "experiment-velocity",
    term: "Experiment Velocity",
    definition:
      "The rate at which an organization designs, launches, analyzes, and acts on experiments, typically measured as the number of experiments concluded per unit time, reflecting the speed of the organization's learning and iteration cycle.",
    explanation:
      "Experiment velocity is a meta-metric that measures the health and throughput of an organization's experimentation program. While individual experiments measure specific product changes, experiment velocity measures the experimentation capability itself. High experiment velocity means more hypotheses tested, more learnings generated, and more opportunities to discover improvements. For growth teams, experiment velocity is often the most important driver of long-term growth because the rate of learning compounds: an organization running 100 experiments per quarter will discover and ship more improvements than one running 10, all else being equal. Companies like Booking.com (running over 1000 concurrent experiments), Netflix, and Microsoft have built competitive advantages through extreme experiment velocity.\n\nExperiment velocity is affected by multiple factors across the experimentation lifecycle. Ideation velocity depends on the team's ability to generate testable hypotheses, which is driven by access to data, customer insight, competitive analysis, and creative problem-solving. Design velocity depends on having experiment templates, automated power analysis, and clear guidelines that reduce the time from hypothesis to launch-ready experiment. Execution velocity depends on feature flagging infrastructure, experimentation platform capabilities, and engineering resources for implementing treatments. Analysis velocity depends on automated statistical analysis, self-serve dashboards, and clear decision criteria. Decision velocity depends on organizational alignment on metrics, decision-making authority, and the speed of ship/no-ship decisions after analysis is complete. Bottlenecks at any stage limit overall velocity.\n\nTeams should measure and track experiment velocity as a key performance indicator for the growth function. Useful sub-metrics include experiments launched per week, median time from hypothesis to launch, median experiment duration, median time from experiment conclusion to ship decision, and the percentage of experiments that produce actionable results. Common pitfalls include optimizing for quantity over quality (running many trivial experiments to inflate the count), not investing in the infrastructure and tooling that enables sustainable velocity, and creating organizational bottlenecks through excessive review processes. The goal is not to maximize the number of experiments but to maximize the rate of validated learning, which requires a balance of quantity, quality, and the speed of acting on results.\n\nAdvanced approaches to improving experiment velocity include automation of the entire experiment lifecycle (auto-generating power analyses, auto-configuring experiment parameters, auto-analyzing results), self-serve experimentation platforms that allow product managers to run experiments without engineering support for certain types of changes, experiment templates that standardize common experiment patterns, and portfolio management approaches that optimize the allocation of experimentation resources across teams and product areas. Some organizations use experiment velocity as an input to their growth model, estimating how many winning experiments are needed per quarter to hit growth targets and working backward to the required experimentation throughput. Machine learning can accelerate velocity by predicting which experiments are most likely to succeed, enabling better prioritization of the experiment backlog.",
    category: "experimentation",
    relatedTerms: ["growth-experimentation-framework", "experiment-review-board", "experiment-documentation"],
    relatedPosts: [],
  },
  {
    slug: "experiment-review-board",
    term: "Experiment Review Board",
    definition:
      "A cross-functional governance body that reviews experiment designs before launch and results before ship decisions, ensuring statistical rigor, alignment with organizational metrics, and prevention of common methodological errors.",
    explanation:
      "An experiment review board (ERB) provides quality assurance for an organization's experimentation program. As experimentation scales beyond a small team of specialists, the risk of methodological errors increases: incorrect power calculations, inappropriate metrics, flawed randomization, peeking without sequential testing, and metric gaming all become more common when many people run experiments with varying levels of statistical sophistication. The ERB provides expert oversight at two critical junctures: before launch (reviewing the experiment design for correctness) and before shipping (reviewing the analysis for validity). For growth teams, an ERB maintains the credibility of the experimentation program by preventing false positives from reaching production and ensuring that experiment designs are set up for success.\n\nAn effective ERB reviews several aspects of each experiment design: the hypothesis clarity and falsifiability, the primary metric selection and its alignment with business goals, the power analysis and expected experiment duration, the randomization unit and its appropriateness for the treatment, potential interactions with concurrent experiments, guardrail metrics that protect against negative side effects, and the pre-registered analysis plan including how multiple comparisons will be handled. Post-experiment, the ERB reviews the sample ratio mismatch check, the primary analysis results and confidence intervals, any deviations from the pre-registered plan, segment analyses and their corrections for multiple testing, and the practical significance of the results relative to implementation costs. The ERB typically includes senior data scientists, product leaders, and engineering representatives.\n\nERBs should be established when the experimentation program reaches a scale where quality control becomes challenging, typically when multiple teams are running experiments independently. The ERB should streamline, not slow down, experimentation by providing quick turnaround reviews (24-48 hours), offering clear guidelines that pre-answer common questions, and using tiered review levels (lightweight review for standard experiments, deep review for high-stakes experiments). Common pitfalls include making the ERB a bottleneck that reduces experiment velocity, staffing it with people who lack statistical expertise, applying a one-size-fits-all review depth regardless of experiment risk, and not providing feedback that helps teams improve their future experiment designs. The ERB should also maintain an experiment knowledge base and publish guidelines that raise the overall level of experimentation literacy in the organization.\n\nAdvanced ERB practices include automated pre-launch checks that catch common errors before human review (insufficient power, missing guardrail metrics, overlap with concurrent experiments), standardized experiment scorecards that make review efficient and consistent, post-mortem analysis of experiments that produced surprising results (both positive and negative) to identify systematic biases in the program, and periodic calibration exercises where ERB members independently review the same experiment to ensure consistency. Some organizations have evolved beyond dedicated ERBs to embedded experimentation expertise within each team, supported by automated tooling that enforces standards programmatically. The meta-goal is to build an organizational culture of experimentation rigor that does not depend on a small group of experts.",
    category: "experimentation",
    relatedTerms: ["experiment-velocity", "experiment-documentation", "growth-experimentation-framework"],
    relatedPosts: [],
  },
  {
    slug: "experiment-documentation",
    term: "Experiment Documentation",
    definition:
      "The systematic recording of experiment hypotheses, designs, configurations, results, and learnings in a structured, searchable format that preserves institutional knowledge and enables evidence-based decision-making across the organization.",
    explanation:
      "Experiment documentation transforms ephemeral test results into durable organizational knowledge. Without documentation, the learnings from each experiment exist only in the memories of the people who ran them, and when those people move on, the knowledge is lost. Teams end up re-testing hypotheses that were already tested, making decisions without considering relevant prior evidence, and failing to build on each other's work. For growth teams, a well-maintained experiment documentation system is a competitive advantage: it enables faster decision-making by surfacing relevant prior results, prevents redundant experiments, identifies patterns across experiments that no single test could reveal, and provides an auditable trail of evidence-based product decisions.\n\nEffective experiment documentation includes several standardized components for each experiment: the hypothesis with clear rationale and predicted outcomes, the experiment design including metrics, randomization unit, sample size, and planned duration, the configuration details (feature flags, variant descriptions, audience targeting), the results including statistical analysis, effect sizes, and confidence intervals for all metrics, the decision made (ship, iterate, or abandon) with reasoning, qualitative observations and unexpected findings, and follow-up actions including any planned iterations. The documentation should be structured and searchable, enabling queries like show me all experiments that tested notification frequency or what experiments have we run targeting new users in the last year. Tools range from purpose-built experiment repositories in platforms like Statsig, Eppo, and Optimizely to internal wikis, Notion databases, or custom-built knowledge management systems.\n\nDocumentation should be required for every experiment, with automated population of design and result fields from the experimentation platform wherever possible to reduce manual burden. The biggest documentation failure mode is requiring too much manual effort, causing teams to skip documentation under time pressure. Common pitfalls include documenting only winning experiments (negative and null results are equally valuable), not recording the reasoning behind decisions (future teams need to understand why a result was interpreted a certain way), using inconsistent formats that make searching and comparison difficult, and not linking related experiments into coherent narratives about a product area. Teams should designate documentation as a core deliverable of every experiment, not an afterthought.\n\nAdvanced documentation practices include automated experiment cataloging that captures design, results, and metadata without manual entry, tagging and taxonomy systems that enable cross-experiment analysis (e.g., browsing all retention experiments, all mobile experiments, or all experiments targeting a specific funnel step), meta-analysis capabilities that aggregate findings across related experiments to produce stronger evidence, and AI-assisted documentation that generates summaries and identifies connections to prior experiments. Some organizations build experiment dashboards that visualize the cumulative impact of all shipped experiments over time, connecting individual experiment results to portfolio-level business outcomes. The documentation system becomes a training resource for new team members and a strategic planning input for identifying high-opportunity areas based on the success rates and effect sizes of past experiments in different product areas.",
    category: "experimentation",
    relatedTerms: ["experiment-review-board", "experiment-velocity", "growth-experimentation-framework"],
    relatedPosts: [],
  },
  {
    slug: "guardrail-metric-testing",
    term: "Guardrail Metric Testing",
    definition:
      "The practice of monitoring a set of critical business metrics during every experiment to detect unintended negative side effects, even when the primary experiment metric shows a positive result, ensuring that optimizing one metric does not degrade overall user experience or business health.",
    explanation:
      "Guardrail metrics are the safety net of experimentation, catching harmful side effects that would be invisible if teams only looked at their primary metric. A change that improves click-through rate might increase page load time, decrease downstream conversion, or increase support tickets. Without guardrail monitoring, these degradations could ship undetected and compound across many experiments. For growth teams, guardrail metrics are essential because the incentive structure of experimentation naturally biases teams toward their primary metric, creating blind spots for other important dimensions of user experience and business health. Establishing a standard set of guardrails that are monitored in every experiment prevents the tragedy of the commons where each team optimizes its own metric at the expense of the overall product.\n\nA comprehensive guardrail metric set typically includes performance metrics (page load time, app crash rate, API latency, error rates), engagement metrics (session length, pages per session, return rate), business metrics (revenue per user, subscription churn rate, support ticket rate), and user experience metrics (rage clicks, form abandonment, accessibility compliance). The guardrail analysis should define thresholds for acceptable degradation: for example, page load time must not increase by more than 100ms, and crash rate must not increase by more than 0.1 percentage points. When a guardrail is breached, the experiment should be flagged for review regardless of primary metric results. The statistical analysis for guardrails can use one-sided tests (testing only for degradation, not improvement) and may use different significance levels than the primary metric, reflecting the asymmetric cost of missing a degradation versus a false alarm.\n\nEvery experiment should monitor the organization's standard guardrail metrics, and high-risk experiments should add domain-specific guardrails. The standard guardrail set should be configured once in the experimentation platform and automatically applied to all experiments. Common pitfalls include not having guardrails at all, having guardrails that are not monitored until after the experiment concludes (by which time damage may already be done), setting thresholds that are too lenient (allowing meaningful degradations) or too strict (flagging every experiment), and not including guardrails that cover cross-team impacts (a change by the growth team might affect metrics owned by the platform team). Teams should also guard against teams that dismiss guardrail violations when their primary metric looks good.\n\nAdvanced guardrail practices include real-time guardrail monitoring with automated experiment pausing when critical thresholds are breached, heterogeneous guardrail analysis that checks for degradation in specific user segments even when the aggregate guardrail is clean, guardrail dashboards that provide a cross-experiment view of which guardrails are most frequently violated (indicating systemic issues), and tiered guardrail systems that distinguish between hard guardrails (automatic experiment kill if breached) and soft guardrails (require human review). Some organizations implement guardrail budgets that track the cumulative degradation across all experiments, recognizing that many small individually acceptable degradations can compound into a significant overall decline. The relationship between primary metrics and guardrails should be reviewed periodically to ensure the guardrail set reflects current business priorities.",
    category: "experimentation",
    relatedTerms: ["split-testing", "percentage-rollout", "experiment-review-board"],
    relatedPosts: [],
  },
  {
    slug: "long-running-experiment",
    term: "Long-Running Experiment",
    definition:
      "An experiment maintained for weeks, months, or even years beyond the standard analysis period to measure the long-term and cumulative effects of a treatment, capturing delayed impacts on retention, revenue, and user behavior that short-term experiments miss.",
    explanation:
      "Long-running experiments address a fundamental limitation of standard experimentation: most experiments are analyzed after 1-4 weeks, but many important effects take months to fully manifest. A change to the onboarding flow might show a modest short-term lift in activation but produce a large improvement in 90-day retention that would be invisible in a two-week experiment. Subscription pricing changes need months to observe renewal behavior. Feature changes may have novelty or primacy effects that take weeks to stabilize. For growth teams, long-running experiments provide the ground truth on whether short-term experiment results actually translate into long-term business impact, serving as a critical calibration mechanism for the entire experimentation program.\n\nRunning a long-term experiment requires careful infrastructure and process planning. The experiment assignment must remain stable over the entire duration, requiring persistent user-to-variant mapping that survives platform updates and user re-registration. The analysis must handle dynamic populations: users who join during the experiment, users who churn and return, and users whose characteristics change over time. Metrics should be tracked as time series, showing how the treatment effect evolves over weeks and months. Survival analysis methods are particularly useful for long-running experiments, modeling the time until events like churn, upgrade, or milestone achievement. The experiment must be protected from organizational pressure to conclude early or reallocate the traffic, which requires executive sponsorship and clear communication about the experiment's purpose and timeline.\n\nLong-running experiments should be used for strategic questions about product direction, pricing changes, algorithmic modifications, and any treatment expected to have cumulative or delayed effects. They are particularly valuable as holdout experiments that measure the aggregate impact of all changes shipped over a period. Common pitfalls include contamination over time as the treatment and control experiences diverge in unexpected ways due to interactions with other product changes, survivor bias if treatment and control groups have different churn rates (the remaining populations become non-comparable), and the opportunity cost of maintaining a control group that does not receive improvements. Teams should plan a predetermined endpoint or a rolling refresh schedule to balance measurement fidelity with user experience fairness.\n\nAdvanced long-running experiment techniques include difference-in-differences analysis within the experiment to measure the incremental effect of specific changes deployed during the experiment period, sequential analysis with alpha spending functions adapted for very long monitoring periods, and machine learning-based heterogeneous treatment effect analysis that identifies which user segments show growing or diminishing effects over time. Some organizations maintain permanent holdout infrastructure with staggered refresh cycles, ensuring continuous long-term measurement capability. The data from long-running experiments also feeds meta-analytic models that estimate the typical ratio between short-term and long-term effects for different types of interventions, enabling better forecasting of long-term impact from short-term results.",
    category: "experimentation",
    relatedTerms: ["holdout-testing", "novelty-effect", "retention-experiment"],
    relatedPosts: [],
  },
  {
    slug: "network-effect-experiment",
    term: "Network Effect Experiment",
    definition:
      "An experiment designed to measure and optimize features that become more valuable as more users adopt them, addressing the unique challenges of testing network-dependent features where individual user value depends on the behavior and adoption of other users.",
    explanation:
      "Network effect experiments tackle one of the hardest problems in online experimentation: testing features whose value depends on how many other users have adopted them. A messaging feature is worthless if no one else uses it; a marketplace is valuable only if both buyers and sellers participate; a social feed improves as more friends contribute content. Standard A/B testing breaks down for network effects because assigning individual users to treatment and control creates an inconsistent network where some users have the feature and their connections do not. For growth teams building social, collaborative, or marketplace products, network effect experiments are essential for validating that new features will achieve critical mass and for measuring the incremental value that each additional user contributes to the network.\n\nExperimenting with network effects requires specialized designs that respect the interconnected nature of user interactions. Cluster randomization assigns entire connected groups (friend circles, companies, geographic communities) to the same variant, ensuring that all users within a cluster share the same experience. Graph cluster randomization partitions the social graph into densely connected communities and randomizes at the community level. Ego-network randomization treats each user and their immediate connections as a cluster. For two-sided marketplaces, geo-randomization assigns entire markets (cities or regions) to variants. The analysis must account for both the direct effect of the feature and the indirect (spillover) effect that propagates through the network. The interference structure, which describes how one user's treatment affects another user's outcome, must be modeled explicitly using methods from the causal inference literature on interference.\n\nNetwork effect experiments should be used when the treatment involves features that facilitate user-to-user interaction, collaboration, communication, or transactions. The key challenge is achieving sufficient statistical power: cluster randomization dramatically reduces the effective sample size, requiring many more clusters than a naive power analysis based on individual users would suggest. Common pitfalls include using individual-level randomization for features with strong network dependencies (which biases the results because control users are partially exposed to the feature through their treated connections), not accounting for the intracluster correlation in the power analysis, ignoring spillover effects that leak between clusters, and underestimating the time required for network effects to manifest. Network effects often have tipping point dynamics where the feature provides little value until adoption reaches a critical threshold, making short experiments particularly problematic.\n\nAdvanced network effect experimentation includes using bipartite experiment designs for two-sided marketplaces (randomizing both buyer and seller side simultaneously), implementing seeding experiments that test different strategies for achieving critical mass in new networks, and using structural models that parameterize the network effect function to predict outcomes at full-scale adoption from partial-adoption experimental data. The concept of interference-aware estimators, developed by researchers like Aronow and Samii, provides unbiased treatment effect estimates under specified interference structures. Some organizations simulate network effects using agent-based models calibrated from experimental data, enabling counterfactual analysis at adoption levels not directly observed in the experiment. The field of network experimentation is rapidly evolving, with new methods being developed at companies like LinkedIn, Meta, and Uber where network effects are central to the product experience.",
    category: "experimentation",
    relatedTerms: ["cluster-randomization", "marketplace-experiment", "virality-testing"],
    relatedPosts: [],
  },
  {
    slug: "marketplace-experiment",
    term: "Marketplace Experiment",
    definition:
      "An experiment conducted in a two-sided or multi-sided marketplace where treatment effects can propagate between buyer and seller sides, requiring specialized experimental designs that account for cross-side interference and equilibrium effects.",
    explanation:
      "Marketplace experiments address the unique challenges of testing changes in platforms that connect two or more participant types: buyers and sellers, riders and drivers, hosts and guests, creators and consumers. The fundamental challenge is that these sides interact through the marketplace mechanism, creating interference that violates the independence assumptions of standard A/B testing. If an algorithm change improves matching for treatment buyers, it may simultaneously worsen matching for control buyers who compete for the same sellers. For growth teams at marketplace companies like Uber, Airbnb, DoorDash, Etsy, and eBay, marketplace experimentation requires specialized techniques that account for these cross-side dynamics and equilibrium effects.\n\nMarketplace experiments can use several design strategies depending on the nature of the treatment. For changes that affect one side independently (e.g., a UI change for buyers that does not affect what sellers see), standard user-level A/B testing on that side may be appropriate if the treatment does not significantly alter the competitive dynamics between treatment and control users. For changes that affect the matching or transaction mechanism (pricing algorithms, search ranking, recommendation engines), geo-randomization assigns entire markets (cities, regions) to variants, ensuring that all participants within a market experience the same treatment. Switchback designs alternate the entire marketplace between treatment and control over time periods. Two-sided randomization simultaneously varies the experience for both sides using a factorial design: some buyers get treatment with some sellers getting treatment, creating four combinations that allow estimation of direct and cross-side effects. The analysis must account for the unique variance structure of marketplace metrics, where outcomes depend on the interaction between both sides.\n\nMarketplace experiments should be designed with careful consideration of the interference structure. The key question is: does the treatment for one user affect outcomes for other users, and if so, through what mechanism? For a seller-side ranking change, competition between sellers means treatment sellers' improved ranking comes at the expense of control sellers. For a buyer-side pricing change, the demand response affects the equilibrium supply available to all buyers. Common pitfalls include ignoring cross-side effects (which biases the treatment effect estimate), using markets that are too small for adequate within-market sample size, failing to account for market-level confounders in geo-experiments, and not considering general equilibrium effects where the treatment changes the market dynamics in ways that the partial equilibrium experiment does not capture.\n\nAdvanced marketplace experimentation includes budget-balanced designs that ensure the total demand or supply remains constant while varying how it is distributed, shadow pricing experiments that compute alternative prices but do not execute them to estimate the price sensitivity without affecting the market, and structural model-based experiments that combine experimental variation with a structural model of marketplace dynamics to predict the general equilibrium impact of treatment at full scale. Regression discontinuity designs can be applied at marketplace boundaries where treatment rules change discontinuously. Interference-aware estimators developed by researchers at Uber, Lyft, and DoorDash account for within-market competition effects using structural or reduced-form models of the interference mechanism. The rapidly growing literature on marketplace experimentation reflects the economic importance and methodological challenge of testing changes in interconnected market systems.",
    category: "experimentation",
    relatedTerms: ["switchback-testing", "cluster-randomization", "network-effect-experiment"],
    relatedPosts: [],
  },
  {
    slug: "bayesian-optimization",
    term: "Bayesian Optimization",
    definition:
      "A sequential decision-making framework that uses a probabilistic model of the objective function to efficiently search for the optimal configuration of parameters, balancing exploration of uncertain regions with exploitation of promising areas.",
    explanation:
      "Bayesian optimization is a sophisticated approach to finding optimal parameter settings when each evaluation is expensive, whether in compute time, user traffic, or opportunity cost. Instead of testing a grid of configurations or using gradient-based methods, Bayesian optimization builds a surrogate model (typically a Gaussian process) of the objective function from observed evaluations, then uses this model to decide which configuration to try next. The model provides both a predicted value and an uncertainty estimate for each untested configuration, enabling intelligent exploration. For growth teams, Bayesian optimization is valuable for tuning continuous parameters like notification frequency, recommendation algorithm weights, pricing levels, and ad bidding strategies, where the parameter space is too large for exhaustive A/B testing and the objective function is smooth but unknown.\n\nThe Bayesian optimization loop operates in four steps: (1) Fit a surrogate model (Gaussian process, random forest, or neural network) to all observed configuration-outcome pairs. (2) Compute an acquisition function that balances exploration and exploitation, such as Expected Improvement (EI), Upper Confidence Bound (UCB), or Thompson Sampling. EI calculates the expected improvement over the current best observation: EI(x) = E[max(f(x) - f(x_best), 0)], which is analytically tractable under a Gaussian process model. (3) Select the configuration that maximizes the acquisition function as the next point to evaluate. (4) Evaluate the objective at the selected configuration (by running an experiment or computing a metric) and add the result to the observation set. This loop repeats until a budget is exhausted or convergence is reached. Implementations include Ax (from Meta), BoTorch, Optuna, and Spearmint.\n\nBayesian optimization should be used when the parameter space is continuous and moderately dimensional (typically up to 20 dimensions), each evaluation is expensive (requiring experiment traffic or significant compute), the objective function is expected to be smooth, and a relatively small number of evaluations must suffice. It is particularly well-suited for tuning ML model hyperparameters, optimizing ad campaign parameters, and finding optimal pricing or notification settings. Common pitfalls include using Bayesian optimization for high-dimensional discrete parameter spaces where it offers little advantage over random search, not accounting for the noisy nature of experiment outcomes (Gaussian processes need noise variance estimation), overfitting the surrogate model in early iterations when few observations are available, and treating the optimization result as definitive without validating the optimal configuration in a proper confirmatory experiment.\n\nAdvanced Bayesian optimization techniques include multi-task and multi-fidelity optimization, which leverages cheap approximations (low-traffic experiments, simulated environments) to guide the search before running expensive full-scale evaluations. Constrained Bayesian optimization handles settings where configurations must satisfy constraints (e.g., latency must remain below a threshold while maximizing click-through rate). Batch Bayesian optimization selects multiple configurations to evaluate in parallel, which is natural for online experimentation where multiple variants can be tested simultaneously. For growth teams, the integration of Bayesian optimization with experimentation platforms enables continuous parameter tuning: rather than running discrete A/B tests for each parameter value, the system continuously proposes and evaluates configurations, converging on optimal settings much faster than manual iteration.",
    category: "experimentation",
    relatedTerms: ["adaptive-experiment", "contextual-bandit-experiment", "epsilon-greedy"],
    relatedPosts: [],
  },
  {
    slug: "frequentist-testing",
    term: "Frequentist Testing",
    definition:
      "The classical statistical hypothesis testing framework used in most A/B tests, where decisions are based on p-values and confidence intervals derived from the sampling distribution of test statistics under the null hypothesis of no treatment effect.",
    explanation:
      "Frequentist testing is the dominant statistical framework for online experimentation, rooted in the Neyman-Pearson hypothesis testing paradigm. The approach formulates a null hypothesis (no treatment effect), computes a test statistic from the observed data, determines how extreme this statistic would be if the null hypothesis were true (the p-value), and rejects the null if the p-value falls below a pre-specified significance level (alpha). For growth teams, frequentist testing provides a well-understood framework with clear decision rules, established sample size formulas, and broad tool support. Most experimentation platforms including Optimizely, VWO, and Google Optimize default to frequentist analysis, and the vast majority of published experiment results use frequentist methods.\n\nThe standard frequentist analysis for an A/B test proceeds as follows: compute the point estimate of the treatment effect (difference in means or proportions between treatment and control), compute the standard error of this estimate, form the test statistic z = effect_estimate / SE, compare z to the critical values from the standard normal distribution (z = 1.96 for a two-sided test at alpha = 0.05), and construct the 95% confidence interval as effect_estimate +/- 1.96 * SE. If the confidence interval excludes zero, the result is statistically significant. For proportion metrics, the test statistic uses the pooled proportion under the null for the variance estimate. For continuous metrics with potentially non-normal distributions, the central limit theorem ensures the test statistic is approximately normally distributed for large samples, which is typically satisfied in online experiments. The power of the test depends on the sample size, the true effect size, the significance level, and the metric variance.\n\nFrequentist testing should be the default analysis method for most online experiments, especially when the organization values standardized decision rules and straightforward interpretation. The framework's main advantages are its simplicity, the pre-experiment control over error rates through power analysis, and the extensive tooling available. Common pitfalls specific to the frequentist approach include misinterpreting the p-value (it is not the probability that the null is true), conflating statistical significance with practical significance, the peeking problem (checking results repeatedly inflates the false positive rate), and the difficulty of incorporating prior information about expected effects. Bayesian testing offers advantages in interpretability (posterior probabilities are more intuitive than p-values), natural handling of sequential monitoring, and the ability to incorporate prior information, but requires specifying prior distributions and can be sensitive to prior choice.\n\nAdvanced frequentist methods for online experimentation include sequential testing procedures (group sequential, always-valid p-values) that allow valid interim monitoring, robust variance estimation methods that handle heavy-tailed metric distributions common in digital data (winsorized estimators, trimmed means), stratified analysis that improves precision by accounting for known sources of variation, and CUPED-style covariate adjustment. The debate between frequentist and Bayesian approaches has largely converged in practice: modern experimentation platforms offer both, and the choice often depends on organizational preference and the specific use case. The key insight is that both frameworks answer slightly different questions (frequentist: how unusual is this data if the null is true? Bayesian: what should we believe given this data?) and both can lead to valid decisions when properly implemented.",
    category: "experimentation",
    relatedTerms: ["split-testing", "confidence-interval", "type-i-error", "peeking-problem"],
    relatedPosts: [],
  },
  {
    slug: "minimum-detectable-effect",
    term: "Minimum Detectable Effect",
    definition:
      "The smallest treatment effect that an experiment is designed to detect with a specified level of statistical power, serving as the bridge between statistical capability and practical relevance in experiment planning.",
    explanation:
      "The minimum detectable effect (MDE) is a critical parameter in experiment planning that defines the sensitivity of the test. It answers the question: what is the smallest improvement this experiment can reliably detect? If the MDE is set to a 2% relative change in conversion rate, the experiment is designed to have adequate power (typically 80%) to detect an effect of 2% or larger, but may miss smaller real effects. For growth teams, the MDE is the key input that determines experiment feasibility: it translates between what the team hopes to achieve, what the statistical test can detect, and how much traffic is needed. Setting the MDE too small requires impractically large sample sizes; setting it too large means the experiment can only detect unrealistically huge effects.\n\nThe MDE is calculated from the power analysis formula by solving for the effect size: MDE = (Z_alpha/2 + Z_beta) * sqrt(2 * sigma^2 / n), where sigma^2 is the metric variance, n is the sample size per variant, Z_alpha/2 is the critical value for the significance level, and Z_beta is the critical value for the desired power. For proportion metrics, sigma^2 = p(1-p) where p is the baseline proportion. The MDE has a direct relationship with sample size: halving the MDE requires quadrupling the sample size. Experimentation platforms typically present MDE calculators that take your metric baseline, daily traffic, and desired experiment duration, then output the MDE, or alternatively take the desired MDE and output the required duration. Teams should compute the MDE before every experiment and assess whether it represents a practically meaningful effect.\n\nThe MDE should be set based on the practical significance threshold: what is the smallest effect that would justify shipping the change? This requires input from product and business stakeholders, not just data scientists. If implementing a change costs two weeks of engineering time and adds codebase complexity, a 0.1% conversion improvement may not be worth it, even if detectable. In that case, setting the MDE at 1% and accepting a shorter experiment is more appropriate. Common pitfalls include setting the MDE based on what is achievable with available traffic rather than what is practically meaningful (leading to underpowered experiments that waste traffic on inconclusive results), not considering the MDE when interpreting null results (a null result from an experiment with MDE of 5% does not rule out meaningful 2% effects), and ignoring that the MDE applies to the primary metric, not to secondary or subgroup analyses which have larger effective MDEs.\n\nAdvanced MDE considerations include computing MDEs for ratio metrics (which require the delta method for variance estimation), adjusting MDEs for CUPED or other variance reduction techniques (which reduce the effective MDE for a given sample size), computing MDEs for cluster-randomized experiments (which have larger MDEs due to the design effect), and using meta-analytic data from past experiments to calibrate realistic MDE expectations. Research shows that the distribution of true effects in most experimentation programs is heavily right-skewed, with many small effects and few large ones, meaning that experiments with large MDEs will miss most real effects. This argues for investing in variance reduction, sensitive metrics, and triggered analysis to reduce MDEs and increase the fraction of experiments that produce conclusive results.",
    category: "experimentation",
    relatedTerms: ["power-analysis", "effect-size", "practical-significance"],
    relatedPosts: [],
  },
  {
    slug: "practical-significance",
    term: "Practical Significance",
    definition:
      "The assessment of whether a statistically significant experiment result represents a meaningful business impact that justifies the cost of implementation, maintenance, and complexity of shipping the change, distinct from mere statistical significance.",
    explanation:
      "Practical significance evaluates whether an experiment result matters for the business, regardless of its statistical significance. A result can be statistically significant (unlikely to be due to chance) but practically insignificant (too small to be worth acting on), or statistically insignificant (the experiment was underpowered) but potentially practically significant (the point estimate suggests a meaningful effect). For growth teams, practical significance is the decision-relevant criterion because shipping a change has real costs: engineering effort to implement and maintain, codebase complexity, user experience disruption, and opportunity cost of not pursuing other changes. A statistically significant 0.02% improvement in click-through rate is real, but if implementing it requires a week of engineering time, it is not worth shipping.\n\nDetermining practical significance requires pre-specifying the minimum effect size of interest (MESOI) before the experiment. The MESOI should reflect the business context: the cost of implementation, the revenue impact at scale, the strategic importance of the metric, and the opportunity cost of engineering resources. For a large-scale product with millions of daily users, a 0.1% conversion improvement might generate millions in annual revenue and be highly practically significant. For a small feature used by thousands of users, a 5% improvement might be necessary to justify the investment. The analysis should compare the confidence interval to the MESOI: if the entire CI is above the MESOI, the result is both statistically and practically significant. If the CI includes the MESOI but also extends below it, the result is ambiguous. If the CI is entirely below the MESOI (even if above zero), the result is statistically significant but not practically significant.\n\nTeams should establish practical significance thresholds as part of their experiment planning process and document them in the analysis plan. This prevents the common failure mode of post-hoc rationalization, where any statistically significant result is declared a win regardless of its magnitude. Common pitfalls include not setting practical significance criteria before the experiment, confusing statistical significance with business impact, celebrating small relative effects that sound impressive but have trivial absolute impact (a 50% lift from 0.01% to 0.015%), and ignoring the confidence interval width when making ship decisions. Equivalence testing provides a formal framework for concluding that a treatment is practically equivalent to the control: if the entire CI falls within the MESOI bounds around zero, the treatment can be declared non-inferior.\n\nAdvanced practical significance frameworks include decision-theoretic approaches that formally model the costs and benefits of shipping versus not shipping, incorporating the full posterior distribution of the treatment effect and the cost structure of implementation. Expected value of information (EVOI) calculations determine whether running an experiment is worthwhile in the first place, given the prior uncertainty about the effect size and the cost of the experiment. For organizations running many experiments, practical significance thresholds can be calibrated against the historical distribution of effect sizes to ensure they are realistic. Some teams use a lift-effort matrix that plots the estimated effect size against the implementation effort to prioritize which winning experiments to ship, recognizing that not all statistically and practically significant results deserve equal priority.",
    category: "experimentation",
    relatedTerms: ["effect-size", "minimum-detectable-effect", "confidence-interval"],
    relatedPosts: [],
  },
  {
    slug: "randomization-unit",
    term: "Randomization Unit",
    definition:
      "The entity (user, session, page view, device, cluster, or geographic region) at which random assignment to experiment variants occurs, determining the independence structure of the data and affecting both the validity and statistical power of the experiment.",
    explanation:
      "The randomization unit is one of the most consequential design decisions in experiment planning. It determines at what level random assignment happens and therefore at what level outcomes are independent. The most common randomization unit in online experiments is the user (identified by a persistent ID), but alternatives include session (each visit is independently randomized), page view (each request is independently randomized), device (for cross-platform consistency), cookie (as a proxy for user when login is not required), company or organization (for B2B products), and geographic region (for marketplace or infrastructure experiments). For growth teams, choosing the wrong randomization unit can either invalidate the experiment through interference violations or waste statistical power through unnecessary variance.\n\nThe choice of randomization unit should match the level at which the treatment is experienced and the level at which outcomes are meaningfully independent. User-level randomization is appropriate when the treatment provides a consistent experience across sessions and when the user's outcome is independent of other users' assignments. Session-level randomization provides more independent observations (increasing power) but means a user might see different variants in different sessions, which is inappropriate for experiments testing persistent features or where learning effects span sessions. Page-view-level randomization is useful for testing layout or content presentation changes where each page view is an independent opportunity, but creates a confusing experience if the same user sees different variants within a single session. For B2B products, randomizing at the company level ensures all users within an organization have a consistent experience, which is essential for collaborative features but dramatically reduces the effective sample size.\n\nThe randomization unit directly affects statistical power and analysis. If the unit is the user, the sample size for power analysis is the number of users, and each user contributes one independent observation. If the unit is the session and each user has multiple sessions, there are more independent observations but within-user correlation between sessions must be accounted for (or the standard errors will be underestimated). If the unit is a cluster (company, region), the effective sample size is the number of clusters, which is typically much smaller than the number of individual users. Common pitfalls include using page-view-level randomization for experiments that should be user-level (creating inconsistent experiences), not accounting for the correlation structure when the analysis unit differs from the randomization unit, and underestimating the sample size impact of cluster-level randomization.\n\nAdvanced randomization unit considerations include using multiple randomization layers for different types of experiments (user-level for UI changes, session-level for content ranking, region-level for pricing), implementing ID resolution to ensure consistent randomization across devices for the same user, and handling anonymous-to-logged-in transitions where the randomization unit changes mid-session. Some experimentation platforms support adaptive randomization units that start at a finer grain (page view) and coarsen to user-level once the user is identified. For network experiments, the randomization unit might be a community or cluster identified through graph partitioning algorithms. The choice of randomization unit also affects the interference structure: user-level randomization assumes users do not influence each other, which may be violated in social or marketplace products, necessitating cluster or switchback designs.",
    category: "experimentation",
    relatedTerms: ["cluster-randomization", "split-testing", "sample-ratio-mismatch"],
    relatedPosts: [],
  },
  {
    slug: "experiment-analysis-plan",
    term: "Experiment Analysis Plan",
    definition:
      "A pre-registered document specifying the hypothesis, primary and secondary metrics, statistical methods, sample size, analysis timeline, and decision criteria for an experiment, written before the experiment launches to prevent post-hoc rationalization and p-hacking.",
    explanation:
      "An experiment analysis plan (EAP) is the pre-commitment device that ensures experiment rigor by documenting all analysis decisions before any data is observed. By specifying the hypothesis, metrics, statistical methods, and decision criteria in advance, the EAP prevents the garden of forking paths problem where analysts make data-dependent choices that inflate false positive rates. For growth teams, EAPs are the single most important process improvement for experiment quality because they eliminate the most common source of invalid results: post-hoc analysis decisions that are unconsciously influenced by the desire to find a significant result.\n\nA comprehensive EAP includes the following components: (1) Hypothesis: a clear, falsifiable statement of what change is expected and why. (2) Primary metric: the single metric that determines the ship/no-ship decision, with its baseline value and expected variance. (3) Secondary metrics: additional metrics that provide context and understanding but do not determine the decision. (4) Guardrail metrics: metrics that must not degrade, with specified thresholds. (5) Randomization design: the randomization unit, allocation ratio, and any stratification. (6) Sample size and duration: based on power analysis for the primary metric at the specified MDE. (7) Statistical method: the test to be used (z-test, t-test, sequential, Bayesian), the significance level, and whether one-sided or two-sided. (8) Multiple comparison strategy: how corrections will be applied for secondary metrics and subgroup analyses. (9) Pre-specified subgroups: any planned segment analyses with their rationale. (10) Decision criteria: what results would lead to shipping, iterating, or abandoning.\n\nEvery experiment should have an EAP, with the depth of detail proportional to the experiment's stakes. For low-risk UI experiments, a brief template-based EAP with primary metric, sample size, and significance level may suffice. For high-stakes experiments like pricing changes or major feature launches, a detailed EAP reviewed by the experiment review board is essential. Common pitfalls include writing the EAP after seeing preliminary results (defeating its purpose), specifying the EAP so vaguely that it does not actually constrain the analysis, including so many pre-specified subgroups that the analysis becomes a multiple testing nightmare, and not following the EAP when the results are surprising. The EAP should be stored in the experiment documentation system with a timestamp proving it was created before the experiment launched.\n\nAdvanced EAP practices include registering EAPs in a version-controlled system that provides cryptographic timestamps, incorporating decision trees that specify actions for different outcome scenarios (e.g., if primary metric is significant but guardrail is degraded, escalate to review board), specifying sensitivity analyses that will be conducted to assess robustness (e.g., analyzing with and without outlier removal), and defining a process for EAP amendments if the experiment encounters unexpected issues (like an SRM) that require analysis modifications. Some organizations use templated EAPs that auto-populate from the experimentation platform's configuration, reducing manual effort while ensuring completeness. The clinical trials community has decades of experience with pre-registration through ClinicalTrials.gov, and digital experimentation teams can learn from their practices, including the distinction between protocol amendments (legitimate changes documented transparently) and protocol deviations (unplanned departures that must be reported).",
    category: "experimentation",
    relatedTerms: ["experiment-review-board", "experiment-documentation", "growth-experimentation-framework"],
    relatedPosts: [],
  },
  {
    slug: "stratified-randomization",
    term: "Stratified Randomization",
    definition:
      "A randomization technique that first divides the user population into homogeneous subgroups (strata) based on important characteristics, then randomizes independently within each stratum to ensure treatment groups are balanced on known confounders and to improve statistical precision.",
    explanation:
      "Stratified randomization ensures that experiment groups are balanced on important user characteristics, reducing the risk of accidental imbalance that can bias results and increasing statistical precision. While simple random assignment produces balanced groups on average, any single experiment may have meaningful imbalance by chance, especially with small sample sizes. By stratifying on characteristics like platform (iOS vs. Android), geography (US vs. international), user tenure (new vs. existing), or subscription status (free vs. paid), the randomization guarantees exact or near-exact balance within each stratum. For growth teams, stratified randomization is particularly valuable when the stratification variables are strong predictors of the outcome metric, because it reduces within-group variance and increases statistical power.\n\nThe implementation of stratified randomization involves defining strata based on user characteristics, then performing independent random assignment within each stratum. For example, with two strata (mobile and desktop) and a 50/50 split, exactly 50% of mobile users are assigned to treatment and 50% to control, and independently, 50% of desktop users are assigned to each. This guarantees that the treatment and control groups have identical platform composition. The analysis should account for the stratification through stratified estimation: the overall treatment effect is a weighted average of within-stratum effects, weighted by stratum size. The stratified estimator has lower variance than the unstratified estimator when the stratum means differ, with variance reduction proportional to the between-stratum variation as a fraction of total variation. Experimentation platforms like Statsig and Eppo support stratified randomization natively, and most hash-based randomization systems can incorporate stratification through layered hashing.\n\nStratified randomization should be used when there are known strong predictors of the outcome that could create accidental imbalance, when the experiment has a relatively small sample size where random chance could produce meaningful imbalance, and when certain strata are particularly important for the analysis (e.g., the team plans to analyze mobile and desktop separately). Common pitfalls include stratifying on too many variables (which creates many small strata and can make the randomization complex), stratifying on post-randomization variables (which is impossible and conceptually invalid), not accounting for the stratification in the analysis (which produces valid but less efficient estimates), and confusing stratification with blocking in factorial designs. The number of strata should be kept manageable, typically using 2-8 strata defined by 1-3 categorical variables.\n\nAdvanced stratification methods include covariate-adaptive randomization methods like Pocock-Simon minimization, which sequentially assigns users to maintain balance across multiple covariates simultaneously, even when the number of covariates makes full stratification impractical. Re-randomization methods check whether a proposed random assignment achieves acceptable covariate balance and re-draw if it does not, providing guaranteed balance at the cost of modifying the randomization distribution. For large-scale online experiments, the practical benefit of stratification diminishes because random assignment produces near-perfect balance with large samples, but stratified analysis (analyzing within strata and combining) still provides variance reduction benefits even without stratified randomization. The combination of stratified randomization with CUPED variance reduction provides compounding power improvements.",
    category: "experimentation",
    relatedTerms: ["randomization-unit", "cuped-variance-reduction", "split-testing"],
    relatedPosts: [],
  },
  {
    slug: "adaptive-experiment",
    term: "Adaptive Experiment",
    definition:
      "An experiment design that modifies its parameters during execution based on accumulating data, including adjusting traffic allocation between variants, dropping underperforming arms, or modifying the sample size, while maintaining statistical validity through appropriate corrections.",
    explanation:
      "Adaptive experiments depart from the fixed design of traditional A/B tests by allowing the experiment to evolve as data accumulates. The most common adaptations include response-adaptive randomization (shifting more traffic to better-performing variants), sample size re-estimation (adjusting the planned sample size based on interim effect size estimates), and arm dropping (eliminating clearly inferior variants to focus traffic on promising ones). For growth teams, adaptive experiments offer practical advantages: they reduce the exposure of users to inferior experiences, reach conclusions faster when effects are large, and efficiently handle multi-arm tests where some variants are clearly worse than others. The trade-off is increased statistical complexity and the need for careful correction to maintain valid inference.\n\nThe most widely used adaptive design in online experimentation is multi-armed bandit optimization, which gradually shifts traffic from underperforming variants to better ones using algorithms like Thompson Sampling, Upper Confidence Bound (UCB), or epsilon-greedy. Thompson Sampling maintains a posterior distribution for each variant's reward rate and assigns each user to the variant with the highest random draw from its posterior, naturally balancing exploration and exploitation. Group sequential designs allow early stopping for efficacy (the treatment is clearly better) or futility (the treatment is unlikely to achieve significance if continued) at pre-planned interim analyses. Sample size re-estimation, using methods like the Chen-DeMets-Lan approach, allows the planned sample size to be increased if the interim effect size is smaller than assumed in the original power calculation, without inflating the Type I error rate.\n\nAdaptive experiments should be used when the cost of assigning users to inferior variants is high (e.g., testing multiple landing page designs where each poor design represents lost conversions), when the experiment has many arms (making fixed allocation inefficient), or when there is genuine uncertainty about the expected effect size that makes fixed sample size planning difficult. Common pitfalls include using bandit algorithms when the goal is precise effect estimation rather than reward maximization (bandits optimize for reward but provide biased effect estimates), not implementing the statistical corrections necessary for valid inference under adaptive designs, making too many adaptations too quickly (before enough data accumulates for reliable decisions), and using adaptive designs as an excuse to avoid proper experiment planning.\n\nAdvanced adaptive designs include Bayesian adaptive trials that use predictive probability to guide adaptations, platform trials that allow new arms to be added while the experiment is running (useful for iterating on designs), and response-adaptive randomization with covariate adjustment that personalizes the allocation probability based on user characteristics. The integration of adaptive experiments with machine learning enables sophisticated personalization: rather than finding the single best variant, the system learns which variant is best for which type of user. Experimentation platforms like Statsig offer built-in support for adaptive experiments with appropriate statistical corrections. The key challenge remains balancing the practical benefits of adaptation against the statistical complexity it introduces, and teams should default to fixed designs unless there is a compelling reason for adaptation.",
    category: "experimentation",
    relatedTerms: ["epsilon-greedy", "contextual-bandit-experiment", "bayesian-optimization"],
    relatedPosts: [],
  },
  {
    slug: "epsilon-greedy",
    term: "Epsilon-Greedy",
    definition:
      "A simple exploration-exploitation algorithm used in multi-armed bandit experiments that exploits the current best-performing variant with probability (1-epsilon) and explores by randomly selecting any variant with probability epsilon, where epsilon is typically a small value like 0.1.",
    explanation:
      "Epsilon-greedy is the simplest bandit algorithm for online experimentation, providing a straightforward approach to balancing exploration (learning about variants) with exploitation (showing users the best variant found so far). With probability (1-epsilon), the algorithm serves the variant with the highest observed reward rate; with probability epsilon, it randomly selects any variant including potentially inferior ones. This ensures continuous exploration while primarily serving the best option. For growth teams, epsilon-greedy is often the entry point into adaptive experimentation because it is easy to understand, implement, and debug, making it suitable for teams new to bandit methods who want to move beyond fixed A/B testing.\n\nThe implementation of epsilon-greedy is straightforward: maintain a running estimate of each variant's reward rate (e.g., conversion rate). For each incoming user, generate a random number between 0 and 1. If the number is less than epsilon, randomly select a variant with equal probability (explore). Otherwise, select the variant with the highest estimated reward rate (exploit). After the user's outcome is observed, update the variant's reward estimate. The choice of epsilon controls the exploration-exploitation tradeoff: higher epsilon means more exploration (slower convergence but more robust), lower epsilon means more exploitation (faster focus on the best variant but risk of premature convergence). A common variant is epsilon-decreasing, where epsilon starts high and decays over time: epsilon_t = epsilon_0 / (1 + decay_rate * t). This ensures heavy exploration early when estimates are uncertain, transitioning to near-pure exploitation as confidence grows.\n\nEpsilon-greedy should be used when a simple, interpretable bandit algorithm is sufficient, when the number of arms is small (2-5), and when the exploration rate can be set based on domain knowledge. Common pitfalls include setting epsilon too low (insufficient exploration leading to premature lock-in on a locally optimal variant), not decaying epsilon over time (wasting traffic on unnecessary exploration after the best variant is clear), treating epsilon-greedy results as valid causal estimates (bandit allocation creates biased samples that require correction for inference), and using epsilon-greedy when a more sophisticated algorithm like Thompson Sampling would be substantially more efficient. For experiments where precise causal inference is needed, epsilon-greedy with a fixed allocation ratio (e.g., epsilon = 0.5, which is equivalent to uniform random assignment) may be more appropriate than a heavily exploitative setting.\n\nAdvanced epsilon-greedy variants include contextual epsilon-greedy, which conditions the exploitation decision on user features (choosing the best variant for each user type rather than the globally best variant), epsilon-first (pure exploration for a fixed initial period, then pure exploitation), and adaptive epsilon methods that adjust the exploration rate based on the confidence in current estimates. While epsilon-greedy is theoretically suboptimal compared to Thompson Sampling and UCB (its regret grows linearly rather than logarithmically), the difference is often small in practice for the sample sizes and number of arms typical in digital experiments. For teams that value simplicity and interpretability, epsilon-greedy provides most of the benefit of bandit methods with minimal implementation complexity.",
    category: "experimentation",
    relatedTerms: ["adaptive-experiment", "contextual-bandit-experiment", "bayesian-optimization"],
    relatedPosts: [],
  },
  {
    slug: "contextual-bandit-experiment",
    term: "Contextual Bandit Experiment",
    definition:
      "An adaptive experiment that uses user context (features like demographics, behavior history, and session attributes) to personalize which treatment variant each user receives, learning a policy that maps user characteristics to optimal treatments in real time.",
    explanation:
      "Contextual bandit experiments extend multi-armed bandits from finding the single best variant to finding the best variant for each type of user. Instead of learning that Variant B is best overall, a contextual bandit might learn that Variant A is best for mobile users under 30 and Variant B is best for desktop users over 40. The algorithm uses user context (features) to personalize the variant assignment, continuously updating its policy as it observes outcomes. For growth teams, contextual bandits represent the frontier of experimentation, bridging the gap between traditional A/B testing (which finds one winner for all) and full personalization (which requires knowing the optimal experience for each user). They enable real-time personalized optimization of landing pages, notification strategies, recommendation presentations, and pricing.\n\nContextual bandit algorithms maintain a model that predicts the expected reward of each variant given the user's context features. Popular algorithms include LinUCB (which uses a linear model with upper confidence bound exploration), contextual Thompson Sampling (which samples from the posterior distribution of a Bayesian linear model), and neural contextual bandits (which use deep learning for the reward model). The workflow is: (1) User arrives with context features X (device type, geography, session count, etc.). (2) The algorithm predicts the expected reward for each variant given X. (3) The algorithm selects a variant balancing predicted reward with exploration needs. (4) The user receives the variant and their outcome is observed. (5) The model is updated with the new observation. Platforms like Statsig, Vowpal Wabbit, and custom implementations at companies like Netflix and Spotify support contextual bandit experiments. The offline evaluation of contextual bandit policies uses inverse propensity scoring to estimate how a new policy would have performed on historically logged data.\n\nContextual bandit experiments should be used when there is strong reason to believe that treatment effects vary across user segments, when the number of variants is manageable (typically 2-10), and when the context features are available at decision time. The most common applications include personalizing which email subject line to send, which landing page to show, which product recommendation layout to use, and which notification frequency to set. Common pitfalls include using context features that are noisy or not available in real time, overfitting the reward model to spurious patterns in early data, not accounting for delayed rewards (a user who converts three days later), and treating the bandit policy as a black box without understanding what context-variant mapping it has learned.\n\nAdvanced contextual bandit methods include batched Thompson Sampling for environments where model updates happen periodically rather than after each user, off-policy evaluation techniques like doubly robust estimation that evaluate candidate policies using historical data, non-stationary contextual bandits that adapt to changing user behavior over time, and meta-learning approaches that transfer knowledge across similar experimentation contexts. The theoretical framework for contextual bandits connects to heterogeneous treatment effect estimation from causal inference: the optimal policy is equivalent to assigning each user to the treatment with the highest conditional average treatment effect given their context. For organizations building sophisticated personalization systems, contextual bandits provide the experimentation framework for continuously learning and deploying personalized experiences at scale, closing the loop between experimentation and production machine learning.",
    category: "experimentation",
    relatedTerms: ["epsilon-greedy", "adaptive-experiment", "bayesian-optimization", "causal-forest"],
    relatedPosts: [],
  },
];
